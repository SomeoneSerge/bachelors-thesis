\documentclass[a4paper]{article}
% \documentclass[14pt,a4paper,draft]{extarticle}


% \usepackage[left=30mm,right=15mm,top=25mm,bottom=20mm]{geometry}
\usepackage{marginnote}
\usepackage{hyperref}
\usepackage{tabularx}
% ----
\usepackage[russian,english]{babel}
\usepackage{fontspec}
\setmainfont{CMU Serif}
% \setmainfont[SizeFeatures={Size=14}]{CMU Serif}


\usepackage{amsmath,amsthm,amssymb,euscript,calc,graphicx,ifthen,textcomp,rotating,cmap,xstring,etoolbox,xparse,l3regex,xargs,changepage,tocloft,titlecaps,enumitem,upgreek}
\usepackage{mathrsfs}


 \newtheorem{thm}{Theorem}
 \newtheorem{thme}{Theorem}
 \newtheorem{lem}{Lemma}
 \newtheorem{crl}{Corollary}
 \newtheorem{prop}{Proposition}
 \newtheorem{stm}{Statement}
 \newtheorem{dfn}{Definition}
 \theoremstyle{definition}
 \newtheorem{exm}{Example}
 \newtheorem{remn}{Remn}
 \newtheorem*{rem}{Remark}
 \newtheorem*{reme}{Remark}


\usepackage{biblatex}
\addbibresource{thesis.bib}
% \renewcommand{\baselinestretch}{1.3}

\begin{document}

\begin{center}
  \textbf{ВОРОНЕЖСКИЙ ГОСУДАРССТВЕННЫЙ УНИВЕРСИТЕТ}
\end{center}

\begin{flushright}
  На правах рукописи?
\end{flushright}

\vfill

\begin{center}
  \textsc{\Large
    Козлуков\\
    Сергей Викторович
  }
  \\[1.5cm]
  \textbf{SPECTRAL PROPERTIES OF CERTAIN PERTURBED BLOCK MATRICES}\\[1.5cm]
  01.01.01 -- вещественный, комплексный и функциональный анализ?\\[1.5cm]
  Бакалаврская \textbf{ДИПЛОМНАЯ РАБОТА}
\end{center}

\vfill
\begin{flushright}
  Научный руководитель\\
  доктор физико-математических наук\\
  профессор Баскаков~А.~Г.
\end{flushright}
 
\vfill
\begin{center}
  Воронеж~--~2018
\end{center}

\pagestyle{empty}
\newpage

\setcounter{tocdepth}{2}
\tableofcontents
\newpage

% \begin{abstract}
%   This work is designated to the study of certain matrices' spectra,
%   especially some graphs' matrices spectra~\cite{van2003graphs,cvetkovic1980spectra}.
%   We also apply the method of similar operators~\cite{baskakov1983methods,baskakov2017method,baskakov2013completeness}
%   to estimate the eigenvalues and eigenvectors of such matrices.
%   In particular we consider the adjacency matrix of the ``almost-complete-digraph''~\cite{Koz17,sergekozlukov@vspu},
%   a ``tiled matrix''~\cite{Koz18,sergekozlukov@currentproblems}, and Kronecker~\cite{Koz18,sergekozlukov@currentproblems,bellman-matrices-kron,XIANG2005210}
%   products of squared matrices
%   which correspond to the adjacency matrices of NEPS sums of graphs.
% \end{abstract}

\section*{Notation}

We employ the following notation throughout the work:

\( f: X\to Y: x\mapsto f(x) \) --- a function \( f \)
with the domain set \( X \) and the target set \( Y \)
mapping each \( x\in X \) into corresponding \( y=f(x) \in Y\);

\( \mathbb{N} = \{ 1, 2, \ldots \}\) --- the set of all natural numbers without zero;

\( \mathbb{Z} \) --- the set of integers;

\( \mathbb{R} \) --- the field of real numbers.

We use the notation \( a..b \) for ranges so that an open interval \( (a..b) \)
can be distinguished from the tuple \( (a, b) \):

\( [a..b] = \{ x\in\mathbb{R}:\ a\leq x\leq b\} \) --- the closed segment from
\( a \) to \( b \);

\( [a..b) = \{ x\in\mathbb{R}:\ a\leq x < b\} \) --- the half-open interval from
\( a \) to \( b \);

\( (a..b) = \{ x\in\mathbb{R}:\ a < x < b\} \) --- the open interval between
\( a \) and \( b \);

\( \mathbb{R}_+ = [0,+\infty) \) --- the set of nonnegative reals;

\( \mathbb{J} \) --- either \( \mathbb{R} \) or \( \mathbb{R}_+ \);

\( \mathbb{C} \) --- the field of complex numbers;

\( \mathbb{K} \) --- either \( \mathbb{R} \) or \( \mathbb{C} \);

\( M{\times}N = \{ (m, n):\ m{\in}M,\ n{\in}N \} \) with \( M \) and \( N \) sets
denotes their Cartesian product;

\( I \) denotes the identity operator and \( E \) the identity matrix;

\( \sigma(A) \) --- the spectrum of the operator or matrix \( A \);

\( \rho(A) \) --- the resolvent set;

\( \mathtt{Hom}(\mathscr{X}, \mathscr{Y})\) --- the Banach space of bounded
linear operators defined on \( \mathscr{X} \) with the values in \( \mathscr{Y} \).

\( \mathtt{End}\mathscr{X} \) --- Banach algebra of bounded linear operators on
\( \mathscr{X} \);

\newpage

\section*{Introduction}
\newpage

\section{Elements of graph spectra theory}
\newpage

\section{Introduction to the method of similar operators}
\newpage

\section{Almost complete graph adjacency matrix}
\sloppy
Рассмотрим матрицу \( \mathscr{A}_{MN} \) размера \( N\times N \),
 составленную из \( M \) нулей и \( N^2 - M \) единиц.
Как матрица смежности, \( \mathscr{A}_{MN} \) соответствует орграфу,
 полученному из полного графа с~петлями на \( N \) вершинах
 удалением некоторых \( M \) из \( N^2 \) р\"ебер.
Некоторые важные свойства графа связаны с~спектром его матрицы смежностей.
Так, например, в~\cite{wang2003epidemic,chakrabarti2008epidemic} описана дискретная модель
 распространения вируса в~сети, в~которой спектральный радиус матрицы смежностей графа сети
 оказывается пороговым значением \( ^1/_{\tau_0} \) отношения \( ^1/_\tau = ^\delta/_\nu \)
интенсивности~\( \delta \) исцеления инфецированных узлов
 и~интенсивности~\( \nu \) заражения узлов, смежных инфецированным.
 Положение \(^1/\tau\) относительно порога \(^1/_{\tau_0}\) определяет
 (эндемический или эпидемический) характер заражения.
Спектральная теория графов и~е\"е приложения подробно рассмотрены
в~монографии~\cite{cvetkovic1980spectra,cvetkovic2010introduction,godsil2013algebraic}.

Что можно сказать о~собственных значениях матриц рассматриваемого вида?

Матрицу \( \mathscr{A}_{MN} \) можно представить в~виде
\[ \mathscr{A}_{MN} = \mathcal{J}_N - \mathscr{B}_{MN} =
    \begin{pmatrix}1 & \cdots & 1\\ \vdots & \ddots & \vdots \\ 1 & \cdots & 1\end{pmatrix} - \mathscr{B}_{MN}, \]
 где \( \mathcal{J}_N \)~--- матрица, составленная из \( N\times N \) единиц,
 а~\( \mathscr{B}_{MN} \) имеет единицы в~точности на тех \( M \) местах,
 где в~\( \mathscr{A}_{MN} \) стоят нули.

Спектр \( \sigma\left( \mathcal{J}_N \right) \)
 матрицы \( \mathcal{J}_N \) легко считается:
 \( \mathcal{J}_N^2 = N \mathcal{J}_N, \) т.е.
 \( \lambda(\lambda - N) \)~--- аннулирующий и, что легко проверить,
 минимальный многочлен матрицы \( \mathcal{J}_N \), а~значит
 \( \sigma\left( \mathcal{J}_N \right) = \left\{ 0,N \right\}. \)

При достаточно малых \( M \),
 спектры матриц \( \mathcal{J}_N \) и~\( \mathscr{A}_{MN} \) будут ``близки''.
Методом подобных операторов (см.~\cite{baskakov-harmonic,baskakov-split}),
 позволяющим для возмущений ``идеального'' объекта, спектральные свойства которого известны,
 найти элемент рассматриваемой алгебры, изоспектральный возмущ\"енному,
 но имеющий более удобную для вычислений структуру,
 в~статье доказывается
\begin{thm}\label{kozlukovsv:thm:almost-all-ones}
    Пусть \( M < \frac{N^2}{16} \),
    тогда спектр матрицы \( \mathscr{A}_{MN} \) можно представить в~виде
    объединения \( \sigma\left(\mathscr{A}_{MN}\right) = \sigma_1 \cup \sigma_2 \)
    непересекающихся
    одноэлементного множества \( \sigma_1=\{\lambda_1\} \)
    и~множества \( \sigma_2 \), удовлетворяющих условиям:
    \[ \sigma_1 \subset \left\{ \mu\in\mathbb{R}; \lvert \mu - N \rvert < 4\sqrt{M} \right\}, \]
    \[ \sigma_2 \subset \left\{ \mu\in\mathbb{C}; \lvert \mu \rvert < 4\sqrt{M} \right\}. \]
\end{thm}

Доказательство состоит в~построении уравнения для матрицы, подобной \( \mathscr{A}_{MN} \),
 но устроеной ``проще''. Решение возникающего нелинейного уравнения
 в~банаховой алгебре \( \mathtt{Matr}_N\mathbb{C} \)
 доставляется методом простых итераций (см., например,~\cite{baskakov-harmonic}).

Подобие матриц \( \mathcal{A}_1, \mathcal{A}_2 \)
 понимается в~смысле существования обратимой матрицы \( \mathcal{U} \),
 такой что \( \mathcal{A}_1 \mathcal{U} = \mathcal{U} \mathcal{A}_2 \).
Подобные матрицы изоспектральны (их спектры совпадают).

Провед\"ем предварительные преобразования.

\begin{lem}
    Матрица единиц 
    \( \mathcal{J}_N =
    \begin{pmatrix}
        1 & \cdots & 1 \\
        \vdots & \ddots & \vdots \\ 
    1 & \cdots & 1 \end{pmatrix} \),
    подобна матрице
    \[
        \mathcal{A} = \begin{pmatrix}
            N & 0 & \cdots & 0 \\
            0 & 0 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & 0 \end{pmatrix}. \]
    Точнее, существует ортогональная матрица \( \mathcal{U} \),
    такая что
    \( \mathcal{J}_N = \mathcal{U}\mathcal{A} \mathcal{U}^{-1} \).
\end{lem}
\begin{proof}
    Собственному значению \( 0 \) соответствует \( N-1 \) независимый собственный вектор
        \( f_1 = {\left(1,-1,0,\ldots,0\right)}, \ldots,
           f_{N-1} = {\left(0,\ldots,0,1,-1\right)} \),
    а~собственному значению \( N \) матрицы \( \mathcal{J}_N \) 
    соответствует собственный вектор \( f_N = {\left(1,\ldots,1\right)} \).
    Применив ортогонализацию Грамма-Шмидта, получим ортонормальную систему \( h_1, \ldots, h_N \):
    \[
        h_k = \frac{1}{\sqrt{k(k+1)}}
            \left(\smash{\underbrace{1,~\ldots,~1,}_{k \text{ раз}}}~-k,~0,~\ldots,~0\right)
            \in \mathbb{R}^N, \quad k={1, \ldots, N-1} \]
    \[
        h_N = {\left(1,~\ldots,~1\right)} \in \mathbb{R}^N, \]
    В~качестве матрицы \( \mathcal{U} \) выберем матрицу,
    имеющую столбцами векторы \( h_N, h_1, \ldots, h_{N-1} \):
    \[ \mathcal{U} =
    \begin{pmatrix}
        \frac{1}{\sqrt N} &  \frac{1}{\sqrt2} &  \frac{1}{\sqrt{6}} & \cdots & \frac{1}{\sqrt{(N-2)(N-1)}} & \frac{1}{\sqrt{(N-1)N}} \\
        \frac{1}{\sqrt N} & -\frac{1}{\sqrt2} &  \frac{1}{\sqrt{6}} & \cdots & \frac{1}{\sqrt{(N-2)(N-1)}} & \frac{1}{\sqrt{(N-1)N}} \\
        \frac{1}{\sqrt N} & 0                 & -\frac{2}{\sqrt{6}} & \cdots & \frac{1}{\sqrt{(N-2)(N-1)}} & \frac{1}{\sqrt{(N-1)N}} \\
        \frac{1}{\sqrt N} & 0                 &  0                  & \cdots & \frac{1}{\sqrt{(N-2)(N-1)}} & \frac{1}{\sqrt{(N-1)N}} \\
        \vdots            & \vdots            &  \vdots             & \ddots & \vdots                      & \vdots   \\
        \frac{1}{\sqrt N} & 0                 &  0                  & \cdots & \frac{1}{\sqrt{(N-2)(N-1)}} & \frac{1}{\sqrt{(N-1)N}} \\
        \frac{1}{\sqrt N} & 0                 &  0                  & \cdots & \frac{2-N}{\sqrt{(N-2)(N-1)}} & \frac{1}{\sqrt{(N-1)N}} \\
        \frac{1}{\sqrt N} & 0                 &  0                  & \cdots & 0                  & \frac{1-N}{\sqrt{(N-1)N}}
    \end{pmatrix}.\]
\end{proof}

Таким образом, исходная матрица \( \mathcal{A}_{MN} \) подобна матрице
\( \mathcal{A} - \mathcal{B} \), где \( \mathcal{B} = \mathcal{U}^{-1} \mathscr{B}_{MN} \mathcal{U} \).
Далее ортогональность матрицы \( U \) будет играть важную роль.

Матрицы из \( \mathtt{Matr}_N\mathbb{C} \) будем записывать в~блочном виде
\( X \sim
    \begin{pmatrix}
    x_{11} & X_{12} \\
    X_{21} & X_{22}
    \end{pmatrix}, \)
    где \( x_{11} \)~--- число,
    \( X_{12} \)~--- строка, \( X_{21} \)~--- столбец,
    \( X_{22} \)~--- квадратный блок размерности \( N-1 \).
Такие блочные матрицы сами образуют алгебру, изоморфную исходной
и~их можно естественным образом умножать
на элементы пространства \( \mathbb{C}\times\mathbb{C}^{N-1} \),
изоморфного~\( \mathbb{C}^N \):
\[
    \begin{pmatrix}
    x_{11} & X_{12} \\
    X_{21} & X_{22}
    \end{pmatrix}
    \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}
  = \begin{pmatrix}
      x_{11} x_1 + X_{12} x_2 \\
      X_{21} x_1 + X_{22} x_2
      \end{pmatrix},\quad x \sim \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}\in \mathbb{C}\times\mathbb{C}^{N-1}.
    \]
В~дальнейших выкладках изоморфные объекты понимаются взаимозаменяемыми.

Следуя общей схеме метода подобных операторов~\cite{baskakov-split},
будем искать более ``простую'' матрицу, подобную \( \mathcal{A} - \mathcal{B} \),
в~виде \( \mathcal{A} - \mathfrak{J} X \)
с~матрицей преобразования подобия \( E + \Gamma X \),
где \( E\in{\mathtt{Matr}_N\mathbb{C}} \)~--- единичная матрица,
\( \mathfrak{J},\Gamma : \mathtt{Matr}_N\mathbb{C}\to\mathtt{Matr}_N\mathbb{C} \)~--- линейные операторы,
действующие на алгебре \( \mathtt{Matr}_N\mathbb{C} \), подбираемые
в~ходе решения,
      прич\"ем \( \mathfrak{J} \) --- проектор (\(\mathfrak{J}^2=\mathfrak{J}\)),
      ``упрощающий'' возмущение \( \mathfrak{J}X \),
      а \( \Gamma \)
      при всех \( X\in {\mathtt{Matr}_N\mathbb{C}} \) % \( X\sim \begin{pmatrix}x_{11} & X_{12} \\ X_{21} & X_{22}\end{pmatrix} \in {\mathtt{Matr}_N\mathbb{C}} \)
      удовлетворяет уравнению
          \( \mathcal{A}\Gamma X - (\Gamma X) \mathcal{A} = X - \mathfrak{J}X. \)

% Короче, будем решать в~банаховой алгебре матриц порядка \( N \) уравнение
% \begin{equation}\label{kozlukovsv:eq:similarity}
%     (\mathcal{A-B})(E+\Gamma X) = (E+\Gamma X)(\mathcal{A} - \mathfrak{J} X), \quad X\in\mathtt{Matr}_N\mathbb{C}.
%     \end{equation}
% Оператор \( \mathfrak{J} \) обычно выбирают проектором (\(\mathfrak{J}^2=\mathfrak{J}\)).
% \( \Gamma \) определяют поточечно, как решение уравнения
% \( \mathcal{A}\Gamma X - (\Gamma X) \mathcal{A} = X - \mathfrak{J} X, \quad X\in\mathtt{Matr}_N\mathbb{C} \),
% где \( \mathcal{A}\Gamma X - (\Gamma X) \mathcal{A} = \mathtt{ad}_{\mathcal{A}} \Gamma X \),
% \(  \mathtt{ad}_{\mathcal{A}}: \mathtt{Matr}_N\mathbb{C}\to\mathtt{Matr}_N\mathbb{C} \)
% --- оператор коммутирования с \( \mathcal{A} \).
% Ясно, \( \mathcal{A}-\mathfrak{J}X \) имеет тем более простую структуру,
% чем шире ядро проектора \( \mathfrak{J} \).
% Уравнение для \( \Gamma \) в~свою очередь не позволяет сузить ядро слишком сильно.

\begin{lem}
    Операторы \( \mathfrak{J} \) и \( \Gamma \)
    следует задать формулами
    \[
        \mathfrak{J} X = \begin{pmatrix} x_{11} & 0 \\ 0 & X_{22} \end{pmatrix}, \]
    \[
        \Gamma X = \frac{1}{N} \begin{pmatrix} 0 & X_{12} \\ -X_{21} & 0 \end{pmatrix}, \]
        для \( X\sim \begin{pmatrix}x_{11} & X_{12} \\ X_{21} & X_{22}\end{pmatrix} \in \mathtt{Matr}_N\mathbb{C} \).

\end{lem}
\begin{crl}
    Спектр блочно-диагональной матрицы
    \( \mathcal{A} - \mathfrak{J}X = \begin{pmatrix} N - x_{11} & 0 \\ 0 & X_{22} \end{pmatrix} \)
    есть объединение спектров е\"е диагональных блоков:
    \[
        \sigma(\mathcal{A} - \mathfrak{J} X) = \{ N - x_{11} \} \cup \sigma(X_{22}). \]
\end{crl}
\begin{proof}
Пусть \( \Gamma \) действует по формуле
\( \Gamma X = \begin{pmatrix} \Gamma_{11}(X) & \Gamma_{12}(X) \\
                              \Gamma_{21}(X) & \Gamma_{22}(X)
                              \end{pmatrix} \), тогда
\[
    \mathcal{A} \Gamma X - (\Gamma X)\mathcal{A} = 
    \begin{pmatrix} 0 & N\Gamma_{12}(X) \\
        - N\Gamma_{21}(X) & 0
        \end{pmatrix}, \]
и~уравнение для \( \Gamma X \) сводится~к
\[
    X - \mathfrak{J} X =
    N \begin{pmatrix} 0 & \Gamma_{12}(X) \\
        - \Gamma_{21}(X) & 0
        \end{pmatrix}. \]

Значит, \( \mathfrak{J} \) может обнулить в
    \( X \sim
    \begin{pmatrix}
    x_{11} & X_{12} \\
    X_{21} & X_{22}
    \end{pmatrix} \in \mathtt{Matr}_N\mathbb{C} \)
    вс\"е, кроме двух диагональных блоков \( x_{11} \) и \( X_{22} \),
    поэтому положим
\[
    \mathfrak{J} X = \begin{pmatrix} x_{11} & 0 \\ 0 & X_{22} \end{pmatrix}, \]
\[
    \Gamma X = \frac{1}{N}\begin{pmatrix} 0 & X_{12} \\ -X_{21} & 0 \end{pmatrix}. \]
\end{proof}

Теперь выпишем уравнение подобия матриц \( \mathcal{A} - \mathcal{B} \)
и \( \mathcal{A} - \mathfrak{J} X \):
\begin{equation}\label{kozlukovsv:eq:similarity}
    (\mathcal{A-B})(E+\Gamma X) = (E+\Gamma X)(\mathcal{A} - \mathfrak{J} X), \quad X\in\mathtt{Matr}_N\mathbb{C}.
\end{equation}
\begin{lem}
    Уравнение~\eqref{kozlukovsv:eq:similarity} эквивалентно уравнению
    \begin{equation}\label{kozlukovsv:eq:fixptn}
        X = \mathcal{B} \Gamma X + \mathcal{B} - (\Gamma X)(\mathfrak{J}(\mathcal{B} (E + \Gamma X))), \quad X\in\mathtt{Matr}_N\mathbb{C}.
    \end{equation}
\end{lem}
\begin{proof}
Раскрывая скобки, уравнение~\eqref{kozlukovsv:eq:similarity} можно преобразовать к виду
\begin{equation}\label{kozlukovsv:eq:fixptn-ini}
    X = \mathcal{B} \Gamma X + \mathcal{B} - (\Gamma X) \mathfrak{J} X.
\end{equation}
Пусть для \( X \) выполнено~\eqref{kozlukovsv:eq:fixptn-ini}.
Тогда, учитывая равенство \( \mathfrak{J}\left((\Gamma X)\mathfrak{J}X\right) = 0, \)
получим равенство
    \begin{equation}\label{kozlukovsv:eq:jx}
        \mathfrak{J} X = \mathfrak{J}\mathcal{B} + \mathfrak{J}\left(\mathcal{B}\Gamma X\right) = \mathfrak{J}(\mathcal{B} (E + \Gamma X)).
    \end{equation}
Подставляя это выражение обратно в~\eqref{kozlukovsv:eq:fixptn-ini},
    получим~\eqref{kozlukovsv:eq:fixptn}.
Аналогично, применяя к обеим частям равенства~\eqref{kozlukovsv:eq:fixptn} оператор \( \mathfrak{J} \)
    и учитывая, что \( \mathfrak{J}\left( (\Gamma X)\mathfrak{J}(\mathcal{B} (E + \Gamma X)) \right) = 0 \),
    получим~\eqref{kozlukovsv:eq:fixptn-ini}
\end{proof}

Выражение в правой части уравнения~\eqref{kozlukovsv:eq:fixptn} обозначим как
\[
    \Phi(X) = \mathcal{B} \Gamma X + \mathcal{B} - (\Gamma X)(\mathfrak{J}(\mathcal{B} (E + \Gamma X))).\]
Теперь покажем, что, при определ\"енных условиях,
возникшее нелинейное отображение \( \Phi:\mathtt{Matr}_N\mathbb{C}\to \mathtt{Matr}_N\mathbb{C} \) имеет инвариантным множеством
некоторый шар \( \Omega \subset \mathtt{Matr}_N\mathbb{C} \) с~центром в~нуле
(т.е.~\( \Phi(\Omega)\subset\Omega \)),
на котором оно является сжимающим.

Пусть в~\( \mathtt{Matr}_N\mathbb{C} \)
выбрана какая-нибудь субмультипликативная норма \( \|\cdot\| \)
(т.е.~норма, удовлетворяющая неравенству
 \( \| \mathcal{A}_1\mathcal{A}_2 \| \leq \|\mathcal{A}_1\|\|\mathcal{A}_2\| \)
 при всех \( \mathcal{A}_1, \mathcal{A}_2 \in \mathtt{Matr}_N\mathbb{C} \)).
Нам нужно найти такой радиус \( r \geq 0 \),
что при \( \|X\|,\|Y\| \leq r \) выполнялись бы неравенства \( \|\Phi(X)\| \leq r \)
и~\( \|\Phi(X) - \Phi(Y)\| < q\|X-Y\| \), \( q\in(0,1) \).
Обозначим
\( \beta = \|\mathcal{B}\| \), \( \gamma = \sup_{\|X\|=1} \|\Gamma X\| \).

\begin{lem}
    Пусть \( \gamma\beta < \frac14\),
    тогда шар
    \[
        \Omega = \left\{ X\in \mathtt{Matr}_N\mathbb{C}; \|X\| \leq r_0 \right\}, \]
    \[  0 < r_0 = \frac{1 - 2\gamma\beta - \sqrt{1-4\gamma\beta}}{2\gamma^2\beta} < 4\beta, \]
    удовлетворяет условию \( \Phi(\Omega)\subset\Omega \).
\end{lem}
\begin{proof}
Очевидно неравенство
    \[ \| \Phi(X) \| \leq
     \beta \gamma^2 \|X\|^2 + 2\beta\gamma\|X\| + \beta. \]
Значит, если \( r \) удовлетворяет неравенству
    \begin{equation}\label{kozlukovsv:ineq:invariance-radius}
        \beta \gamma^2 r^2 + (2\beta\gamma - 1)r + \beta \leq 0,
    \end{equation}
    то \( \|\Phi(X)\| \leq r \) при всех \( \|X\| \leq r \).
Если \( \gamma\beta \leq \frac14 \),
    то дискриминант \( \Delta = 1-4\gamma\beta \)
    соответствующего уравнения положителен и~его корни вещественны.
Из знаков коэффициентов возникшего многочлена видно, что оба корня положительны.
Следовательно, наименьший положительный \( r \),
    удовлетворяющий неравенству~\eqref{kozlukovsv:ineq:invariance-radius}
    есть наименьший корень
    соответствующего уравнения:
    \[ r_0 = \frac{1 - 2\gamma\beta - \sqrt{1-4\gamma\beta}}{2\gamma^2\beta}. \]
Учитывая \( \gamma\beta<\frac14 \), имеем \( r_0 < 4\beta \).
\end{proof}

Аналогичным образом устанавливается
\begin{lem}
    Пусть \(\gamma\beta<\frac14\),
    тогда \( \Phi \)~--- сжимающее отображение:
    \[ \| \Phi(X) - \Phi(Y) \| \leq q \|X - Y\|, \quad X,Y\in\Omega \]
    \[ q = (1+2\gamma r_0) \gamma\beta \leq (1+8\gamma\beta)\gamma\beta \leq \frac34. \]
\end{lem}
\begin{proof}
    \begin{align*} \| \Phi(X) - \Phi(Y) \| = \| \mathcal{B}\Gamma (X-Y) + (\Gamma X)(\mathcal{B}\Gamma X + \mathcal{B})
     - (\Gamma Y)(\mathcal{B} \Gamma Y + \mathcal{B}) \| \leq \\
        \leq
     \beta\gamma\|X-Y\| +
     \beta \gamma^2 \|X-Y\| \|X+Y\| \leq \\
        \leq
     \beta\gamma\|X-Y\| +
     2 r_0 \beta \gamma^2 \|X-Y\|.
    \end{align*}
Здесь использовано равенство
\[ (\Gamma X) \mathfrak{J}(\mathcal{B}\Gamma X) - (\Gamma Y) \mathfrak{J}(\mathcal{B}\Gamma Y) =
    \frac12\left[
        \Gamma(X-Y) \mathfrak{J}(\mathcal{B}\Gamma(X+Y))
    +   \Gamma(X+Y) \mathfrak{J}(\mathcal{B}\Gamma(X-Y))
    \right]. \]
\end{proof}

Отсюда и~из теоремы Банаха о~неподвижной точке следует:
\begin{lem}
В~шаре \[ \Omega = \left\{ X\in\mathtt{Matr}_N\mathbb{C}; \quad \|X\| \leq r_0 \right\} \]
    существует и~при том единственное решение \( X^o \) уравнения~\eqref{kozlukovsv:eq:fixptn},
    являющееся пределом последовательности \( \{ \Phi^k(0); k\in\mathbb{N} \} \),
    где \( \Phi^k = \Phi\circ\Phi^{k-1} \)~--- композиция.
\end{lem}

\begin{crl}
Матрица \( \mathcal{A} - \mathcal{B} \) подобна блочно-диагональной матрице \( \mathcal{A} - \mathfrak{J} X^o \):
\[ \mathcal{A} - \mathcal{B} \sim
\begin{pmatrix}
N - x_{11}^o & 0 \\
0 & -X_{22}^o
\end{pmatrix}, \]
при этом выполняются условия:
\[ \sigma\left(\mathcal{A} - \mathcal{B}\right) = \left\{N-x_{11}^o\right\}\cup \sigma\left(-X_{22}^o\right), \]
    \[ x_{11}^o\in\mathbb{R}, \lvert x_{11}^o \rvert \leq r_0 \leq 4\beta, \]
\[ \sigma\left(-X_{22}^o\right) \subset \{ \mu\in\mathbb{C}; \lvert x \rvert \leq r_0 \leq 4\beta \}. \]
\end{crl}
\begin{proof}
    Матрица \( \mathcal{A} - \mathcal{B} \) подобна блочно-диагональной \( \mathcal{A} - \mathfrak{J} X^o \),
    поэтому их спектры совпадают.
    Спектр матрицы \( \mathcal{A} - \mathfrak{J} X^o \) есть объединение спектров е\"е диагональных блоков.
    В~виду субмультипликативности нормы имеют место неравенства
    \[ \mathtt{spr}(X^o) = \max_{\lambda\in\sigma(X^o)}\lvert\lambda\rvert \leq \|X^o\| \leq r_0. \]
    Кроме того, собственное значение \( x_{11}^o \) является вещественным, как предел сходящейся вещественной последовательности.
\end{proof}

Верн\"емся, наконец, к~непосредственному доказательству основной теоремы:
\begin{proof}[Доказательство Теоремы~\ref{kozlukovsv:thm:almost-all-ones}]
    Для доказательства осталось лишь выбрать подходящую субмультипликативную норму.
    Заметим, что матрица \( \mathcal{U} \),
    приводящая \( \mathcal{J}_N \) к~диагональному виду
    является ортогональной,
    поэтому умножение на \( \mathcal{U} \) или \(\mathcal{U}^{-1}\)
    есть изометрия.
    Следовательно, \( \|\mathcal{B}\|=\|\mathscr{B}_{MN}\| \).
    Рассмотрим в~пространстве \( \mathtt{Matr}_{N}\mathbb{C} \)
    норму Фробениуса \( {\left\|\cdot\right\|}_{F} \),
    определ\"енную формулой
    \( {\left\|X\right\|}_{F} = \sqrt{\sum_{ij} \lvert x_{ij}\rvert^2}, \)
    \( X = (x_{ij})\in\mathtt{Matr}_N\mathbb{C} \).
    Она субмультипликативна.
    При этом
    \( \mathscr{B}_{MN} \) состоит из \( M \) единиц, поэтому
    \[
        \beta = {\left\|B\right\|}_{F} =
        {\left\|\mathscr{B}_{MN}\right\|}_{F} = \sqrt{M}.
        \]
    Заметим также очевидное равенство
    \[
        \gamma = \frac1N
                \sup_{{\left\|X\right\|}_{F}=1}{\left\|\begin{pmatrix}0 & X_{12} \\ -X_{21} & 0\end{pmatrix}\right\|}_{F}
                = \frac1N. \]
    
    Если
     \( \sqrt{M} < \frac{N}{4} \),
     то выполняются условия леммы,
     прич\"ем \( r_0 < 4\sqrt{M} \).
    Это значит, что
     \( \sigma(\mathscr{A}_{MN}) = \sigma_1 \cup \sigma_2 \),
     где \( \sigma_1 = \{ \lambda_1 \}\subset\mathbb{R}, \lvert \lambda_1 - N \rvert < 4\sqrt{M} \),
     \( \sigma_2 \subset \{ \mu\in\mathbb{C}; \lvert\mu\rvert < 4\sqrt{M} \} \),
     \( \sigma_1 \cap \sigma_2 = \emptyset \).
    Теорема доказана.
    \end{proof}

    % JPCS
    \newpage

\section{TO BE MERGED: almost-all-ones again}

Consider a~directed graph on \( N \) vertices.
It is uniquely defined by its
adjacency matrix
    \( A = (a_{ij}) \)
    of the size \( N\times N \),
    in which \( a_{ij} \)
    is the number of edges
    from the vertex \( i \)
    to the vertex \( j \).
The \emph{simple spectrum} of a graph
    is defined as the spectrum of its adjacency matrix.
Different spectra of a graph are defined in terms of different matrices
    corresponding to this graph.
It is customary to consider combinations
    of the matrices \( A \),
    \( D \) (the out-degree matrix),
    \( E \) (the identity matrix)
    and \( J_N \) (the all-ones matrix).
These matrices arise naturally
    in some stochastic models~\cite[p.~184]{cvetkovic2010introduction}.
Spectral  properties of these matrices
    often play a vital role in such models.
For instance, the Markovian random walk on a~graph
    yields the notion of eigenvector centrality
    in a~network~\cite{ilprints422,bonacich1972factoring}.
The score of a vertex \( i \)
    is defined as the \( i \)'th coordinate
    of a dominating (left) eigenvector
    of the transition matrix of such a process.
This eigenvector
    defines the only stationary distribution
    of this random walk.
The PageRank~\cite{ilprints422} algorithm
    originally used by Google
    to compute the eigenvector centrality
    relies on the Power-Method.
Its speed of convergence depends on
    the ratio of the two largest absolute eigenvalues.
Stability of a stationary distribution
    is determined~\cite{meyer1994sensitivity}
    by the condition number
    which is bounded from below
    by the spectral gap --- the distance between
    the two largest eigenvalues
    of the transition matrix of a process.
The method of estimation of almost-invariant sets
    proposed in~\cite{schwartz2006fluctuation}
    also relies on spectral decompositions of such matrices.
In the Susceptible-Infective-Susceptible model
    a viral spread in a network
    is modeled~\cite{wang2003epidemic,chakrabarti2008epidemic} as a Markov process
    with \( 2^N \) states.
An asymptotic (endemic or epidemic) behaviour of such a system
    is determined by the spectral radius (the largest absolute eigenvalue)
    of the adjacency matrix
    and the rates of curage and infection.

We should also note that the Kronecker products
    of the adjacency matrices
    are themselves of interest
    as they correspond to the adjacency matrices of
    the \emph{non-complete extended p-sum}s~(NEPS)
    of graphs~\cite[p.~44]{cvetkovic2010introduction}.

For more details and comprehensive description
    of the graph spectra theory
    and its applications
    refer to~\cite{cvetkovic1980spectra,cvetkovic2010introduction,godsil2013algebraic}.

\section{The method of similar operators}

In some applications
    it is infeasible to compute the exact eigenvalues.
Then one needs to make reasonable estimates.
We use the abstract method of similar operators
    to estimate the eigenvalues and eigenvectors
    for a certain class of matrices.
This method originates from Friedrichs~\cite{friedrichs1965advanced}
    and was later developed in abstract setting
    by Baskakov~\cite{baskakov1983methods,baskakov2017method,baskakov2013completeness}.
It relies on contraction mappings in Banach spaces
    and the Banach fixed-point theorem.
This approach is often superior to usual methods of perturbation analysis
    that use series expansions.
We are only concerned with finite-dimensional problems
    so we will only state the required notation and theorems
    in a simplified form.

Let \( \mathbb{K}\in \{ \mathbb{R}, \mathbb{C} \} \)
    be a field of either real or complex numbers.
We consider the vector space \( \mathbb{K}^n,\ n\in \mathbb{N} \)
    supplied with Euclidean structure:
    \[
        (x, y){=}\sum_{k=1}^n x_k\overline{y_k},
        \ x{=}(x_1,\ldots, x_n),
        \ y=(y_1,\ldots, y_n)
        \in \mathbb{K}^n
        \]
    and the \( \mathrm{L}_2 \)-norm:
    \(
        \|x\|_2^2{=}(x,x).
        \)
We also consider the canonical basis \( e_1, \ldots, e_n \)
    in \( \mathbb{K}^n \) given by
    \( {(e_i)}_j = \delta_{ij},\ i,j=\overline{1,n} \)
    (\(\delta_{ij} \) is the Kronecker symbol).
When \( V_1, V_2 \) are normed vector spaces
    we denote by \( L(V_1, V_2) \)
    the space of bounded linear mappings
    from \( V_1 \) to \( V_2 \).
An algebra of bounded linear endomorphisms
    from a Banach space \( V \)
    into itself
    is denoted by \( L(V) = L(V, V) \).
It is a Banach algebra with the operator norm:
    \[
        \|A\|_{\mathrm{op}} =
        \sup_{
            \substack{\|x\|=1,\\ x\in V}
        } \|A x\|,\ A\in L(V).
        \]
Together with \( L(\mathbb{K}^n, \mathbb{K}^m) \)
    we consider its isomorphic space \( \mathbb{K}^{m{\times}n} \)
    of matrices of the size \( m{\times}n \)
    with entries from the field \( \mathbb{K} \).
The space \( \mathbb{K}^{n{\times}n}\sim L(\mathbb{K}^n) \)
    forms a Banach algebra
    when supplied with a submultiplicative norm
    \( \|\cdot\| \),
    e.g.: \( \|A\|_{\mathrm{op}} = \sup_{\|x\|_2=1,\ x\in \mathbb{K}^n} \|A x\|_2,\ \)
    \( \|A\|_{\mathrm{F}} = \sqrt{\sum_{i,j} |a_{ij}|^2},\ \)
    for 
    \( A{=}(a_{ij})\in\mathbb{K}^{n\times n} \).
Finally we will also be dealing with the isomorphic spaces
    \( L(L(\mathbb{K}^n)) \) and \( L(\mathbb{K}^{n{\times}n}) \)
    with the operator norm.
We will follow Krein
    and refer to elements of \( L(\mathbb{K}^{n{\times}n}) \)
    as ``transformers''.

The spectrum of a matrix \( A \)
    (the set of its eigenvalues)
    will be denoted as \( \sigma(A) \).
We call two matrices \( A_1, A_2 \) \emph{similar}
    if there is an invertible matrix \( U \)
    (the similarity matrix)
    such that \( A_1 U = U A_2 \).
Similar matrices share some spectral properties:
    they are isospectral (\( \sigma(A_1) = \sigma(A_2) \))
    and \( U \) maps the eigenvectors of one to another's:
    \( A_2 x = \lambda x \implies A_1 U x = \lambda U x \).

The most important notion
    in the abstract method of similar operators
    is that of an \emph{admissible triple}.
For our specific purposes it suffices to say
    that \( (\mathbb{K}^{n{\times}n}, J, \Gamma) \)
    forms an \emph{admissible triple}
    for a matrix \( A\in\mathbb{K}^{n{\times}n} \)
    if the following conditions are met:
\begin{itemize}
    \item \( J, \Gamma \in L(\mathbb{K}^{n{\times}n}) \)
        are transformers;
    \item \( J \) is a projection (\( J^2 = J \));
    \item  \( \Gamma \) satisfies the equations:
        \[
            A \Gamma X - (\Gamma X) A = X - JX,
        \]
        \[
            J\Gamma X = 0,\ X\in\mathbb{K}^{n{\times}n}.
        \]
\end{itemize}

The main theorem of the method
    may be now formulated as follows:

\begin{thm}
    Consider a matrix \( A - B \)
        with \( A, B \in \mathbb{K}^{n{\times}n} \).
    Suppose \( (\mathbb{K}^{n{\times}n}, J, \Gamma) \)
        is an admissible triple for the matrix \( A \)
        and suppose the following inequality holds:
        \[
            \|B\|\|\Gamma\|_{\mathrm{op}} \leq \frac14.
        \]

    Then there exists such a matrix \( X^o\in\mathbb{K}^{n{\times}n} \)
        that \( A - B \) is similar to \( A - J X^o \);
        the similarity matrix is \( E + \Gamma X^o \);
        the following estimates are valid:
        \[
            \|X^o - B\| \leq 3 \left\|B\right\|,
        \]
        \[
            \operatorname{spr}(X^o) \leq \|X^o\| \leq 4 \left\|B\right\|,
        \]
        where \( \operatorname{spr}(X^o) \)
        is the spectral radius of \( X^o \) (the largest absolute eigenvalue).
    Such \( X^o \) can be found as the limit of a convergent sequence
        \( \left( \Phi^k(0);\ k\in\mathbb{N} \right) \)
        in a Banach algebra \( \mathbb{K}^{n{\times}n} \).
        Here \( \Phi \) is a nonlinear contraction mapping
        defined on the ball \( \{X\in\mathbb{K}^{m{\times}n};\ \|X-B\|\leq 3\|B\| \} \)
        and given by
    \[
        \Phi(X) = B\Gamma X - (\Gamma X)J(B + B\Gamma X) + B
    \]
        and \( \Phi^k = \underbrace{\Phi\circ\cdots\circ\Phi}_{k\ \text{copies}} \)
        denotes the composition.
\end{thm}

\section{Almost-complete graph example NOCH EIN WEITER}

Consider a digraph defined by the following adjacency matrix:
\[
    A = J_N - B = \begin{pmatrix}1 & \cdots & 1 \\ \vdots & \ddots & \vdots \\ 1 & \cdots & 1\end{pmatrix} - B,
\]

Here \( J_N \) is the all-ones matrix.
The unity on the intersection
of \( i \)'th row and \( j \)'th column of \( B \)
corresponds to an edge from \( i \) to \( j \)
being absent in the graph.
This example has already been considered in~\cite{sergekozlukov@volgograd}
and here we will reproduce some steps to demonstrate the technique.

One can easily find the minimal annihilating polynomial of \( J_N \)
    to be \( \lambda(\lambda - N) \).
This comes from the fact that \( J_N^2 = N J_N \).
Consequently the spectrum of \( J_N \) is
\[
    \sigma(J_N) = \left\{0, N\right\}.
\]
The only non-zero eigenvalue \( N \) has the corresponding eigenvector
\[
    h_N = \frac{1}{\sqrt{N}} \left(1, \ldots, 1\right)\in\mathbb{R}^N.
\]
The null-space of \( J_N \) is orthogonal to \( h_N \)
    and allows the orthonormal basis:
\[
    h_k = \frac{1}{\sqrt{k(k+1)}} \left(\underbrace{1, \ldots, 1}_{k\ \text{copies}}, -k, 0, \ldots, 0\right),\ k=\overline{1, N-1}.
\]

We conclude that the adjacency matrix \( A = J_N - B \) of this graph
    is similar to \( \mathcal{A} - \mathcal{B} \)
    where \( A \) is a block matrix (subscripts denote block sizes; in what follows throughout this subsection block sizes are the same and will be omitted):
    \[
        \mathcal{A} = \left(\begin{array}{c|c}
        N & \mathbf{0}_{1{\times}(N{-}1)} \\ \hline
            \mathbf{0}_{(N{-}1){\times}1} & \mathbf{0}_{(N{-}1){\times}(N{-}1)}
        \end{array}\right) \in \mathbb{R}^{N{\times}N}
    \]
    and \( \mathcal{B} \) is obtained with a similarity transform:
    \(
        \mathcal{B} = U^{-1} B U \in \mathbb{R}^{N{\times}N}.
    \)
The similarity matrix \( U \) is given by stacking the eigenvectors in columns:
    \[
        U = \operatorname{columns}(h_N, h_1, \ldots, h_{N-1}) =
        \begin{pmatrix}
            \vline & \vline &        & \vline \\
            h_N    & h_1    & \ldots & h_{N-1} \\
            \vline & \vline &        & \vline

        \end{pmatrix}.
    \]

Following the general method,
    we should first construct an admissible triple.
Since \( \mathcal{A} \) is block-diagonal
    it is natural to set \( J \) with the formula
    \[
        JX =
        \left(\begin{array}{c|c}
            x_{11} & \mathbf{0} \\ \hline
            \mathbf{0} & X_{22}
        \end{array}\right)
    \]
    for all block matrices
    \[
        X =
        \left(\begin{array}{c|c}
            x_{11} & X_{12} \\ \hline
            X_{21} & X_{22}
        \end{array}\right)\in\mathbb{K}^{N{\times}N}.
    \]
Then \( A - JX \) is block-diagonal for any \( X \)
    and its spectrum is the union
    of its diagonal blocks' spectra:
    \( \sigma(A - JX) = \{N, \sigma(-X_{22}) \} \).

Now we can find the corresponding \( \Gamma \).
Suppose it is defined by the formula
    \[ \Gamma X = \begin{pmatrix}
        \Gamma_{11}(X) & \Gamma_{12}(X) \\
        \Gamma_{21}(X) & \Gamma_{22}(X)
        \end{pmatrix}.
    \]
Then the equations
    \[
        \mathcal{A} \Gamma X - (\Gamma X)\mathcal{A} =
        N
        \begin{pmatrix}
          0 & \Gamma_{12}(X) \\
          -\Gamma_{21}(X) & 0
        \end{pmatrix} = X - JX,
     \]
and \( J\Gamma X = 0 \) yield the result
    \[
        \Gamma X = \frac{1}{N} \begin{pmatrix} 0 & X_{12} \\ -X_{21} & 0 \end{pmatrix},\ X\in\mathbb{K}^{N{\times}N}.
    \]

The last step is to estimate the norms and apply the theorem.
One can easily check that \( \|\Gamma\|_{\mathrm{op}} = \frac1N \).
It is also apparent that \( \|\mathcal{B}\|_{\mathrm{op}} = \|B\|_{\mathrm{op}} \leq \|B\|_{\mathrm{F}} \)
    since multiplication by the orthogonal matrix \( U \)
    is an isometry in Euclidean space.
The Frobenius norm \( {\|B\|_{\mathrm{F}} = \sqrt{\sum_{ij} b_{ij}^2} = M} \)
    of \( B \)
    reduces to the square root of the number of absent edges.
This directly implies the following

\begin{thm}
    Suppose the number of absent edges is
    \[ M < \frac{1}{16} N^2. \]
    Then the spectrum of the adjacency matrix \( A = J_N - B \)
        can be represented as disjoint union
    \[
        \sigma(A) = \{ N - x_{11}^o \} \cup \sigma_2.
    \]
    The dominating eigenvector of \( A \) is
    \[
        \hat{h}_N = U(E+\Gamma X^o) e_1 =
            h_N - \frac1N (X_{21,(1)}^o h_1 + \cdots + X_{21, (N{-}1)}^o h_{N{-}1}),
    \]
    where \( X_{21,(i)}^o,\ i=\overline{1,N{-}1} \) are the coordinates
    of the vector \( X_{21}^o \).
    Moreover \( \hat{h}_N\in\mathbb{R}^{N} \),
    \( x_{11}^o\in\mathbb{R} \) and \( \sigma_2\subset\mathbb{C} \)
    satisfy the following inequalities:
    \[
        \|\hat{h}_N - h_N\|_2 \leq 4\frac{\sqrt{M}}{N},
    \]
    \[
        \lvert x_{11}^o \rvert,
        \ \sup_{\lambda\in\sigma_2} \lvert\lambda\rvert \leq 4\sqrt{M}.
    \]
\end{thm}

\section{A-tiled matrix}

Now let \( A\in\mathbb{K}^{M{\times}M} \)
    and consider the following (``A-tiled'') block-matrix
    \[
        \mathbb{A} =
        \begin{pmatrix}
            A & \cdots & A \\
            \vdots & \ddots & \vdots \\
            A & \cdots & A
        \end{pmatrix}
        \in\mathbb{K}^{{MN}{\times}{MN}}
    \]
    and the perturbed matrix
    \[
        \mathbb{A} - \mathbb{B},\ \mathbb{B}\in\mathbb{K}^{{MN}{\times}{MN}}.
    \]

\begin{lem}
    Suppose \( A \) is an invertible self-adjoint matrix.
    Then it has \( M \) orthonormal eigenvectors \( h_1, \ldots, h_M \)
    (\(\left\|h_i\right\|_2 = 1,\ i{=}\overline{1,M}\))
    with the corresponding eigenvalues
    \( \lambda_1, \ldots, \lambda_M \neq 0\).
    The spectrum of \( \mathbb{A} \) is
    \[
        \sigma(\mathbb{A}) = \{0\}\cup N\sigma(A) = \{0\} \cup \{N\lambda;\ \lambda\in\sigma(A) \}.
    \]
    Non-zero eigenvalues of \( \mathbb{A} \)
        have corresponding block eigenvectors:
    \[
        f_j = \frac{1}{\sqrt{N}} (h_j, \ldots, h_j)\in \mathbb{K}^{MN},\ j=\overline{1,M}.
    \]
    The null-space of \( \mathbb{A} \)
        has an orthonormal basis:
    \[
        f_{j,k} = \frac{1}{\sqrt{k(k+1)}}
        (
        \underbrace{e_j, \ldots, e_j}_{k\ \text{copies}},
        -ke_j,
        0, \ldots, 0
        ) \in\mathbb{K}^{{MN}{\times}{MN}}
    \]

    The matrix \( \mathbb{A} \) is similar to a block-diagonal matrix:
    \[
        \mathcal{A} =
        \left(\begin{array}{c|c}
            \operatorname{diag}(N\lambda_1,\ldots,N\lambda_M) & \mathbf{0} \\ \hline
            \mathbf{0} & \mathbf{0}
        \end{array}\right)\in\mathbb{K}^{{MN}{\times}{MN}};
    \]
    the similarity transform matrix is
    \[
        U = \operatorname{columns}
        \left(f_1, \ldots, f_M, f_{1,1}, \ldots, f_{1,N{-1}}, \ldots, f_{M,N{-}1}\right).
    \]
\end{lem}

Throughout this subsection
    we will consider block matrices
    of the size \( {MN}{\times}{MN} \)
    in the form
    \[
    X =
        \left(\begin{array}{c|c}
            \begin{matrix}
                x_{11} & \cdots & x_{1M} \\
                \vdots & \ddots & \vdots \\
                x_{M1} & \cdots & x_{MM}
            \end{matrix} &
            \begin{matrix}
                x_{1,M+1} \\
                \vdots \\
                x_{M,M+1}
            \end{matrix} \\ \hline
            \begin{matrix}
                x_{M+1,1} &
                \cdots &
                x_{M+1,M}
            \end{matrix} &
            X_{M+1,M+1}
        \end{array}\right),
    \]
where
\( X_{ij}      {=} x_{ij},
 \ X_{M{+}1,j} {=} x_{M{+}1,j},
 \ X_{i,M{+}1} {=} x_{i,M{+}1} \in \mathbb{K} \)
for \( 1 \leq {i,j} \leq M \),
and
\( X_{M{+}1,M{+}1} \) is a block of the size \( {M(N{-}1){\times}M(N-1)} \).

Just like before we are going to investigate
    the spectral behaviour of \( \mathbb{A} \) under perturbations.
We begin with constructing an admissible triple.
A natural choice for \( J \) in this case is
\[
        J X =
        \left(\begin{array}{c|c}
            \begin{matrix}
                x_{11} &  & 0 \\
                 & \ddots &  \\
                0 &  & x_{MM}
            \end{matrix} &
            \begin{matrix}
                0 \\
                \vdots \\
                0
            \end{matrix} \\ \hline
            \begin{matrix}
                0 & \cdots & 0
            \end{matrix} &
            X_{M+1,M+1}
        \end{array}\right).
\]

\begin{lem}
Suppose \( A \) has a simple spectrum,
    i.e.\ its eigenvalues are pairwise distinct:
    \( \lambda_i\neq\lambda_j \) for all \( 1\leq i{\neq}j \leq M \).

    Then a tuple \( (\mathbb{K}^{{MN}{\times}{MN}}, J, \Gamma) \)
        forms an admissible triple if we define
    \[
        \Gamma X = 
            \frac1n \left(\begin{array}{c|c}
            \begin{matrix}
                0               & \gamma_{12}x_{12} & \cdots & \gamma_{1M}x_{1M} \\
                \gamma_{21}x_{21}  & 0              & \cdots & \gamma_{2M}x_{2M} \\
                \vdots          & \vdots         & \ddots & \vdots & \ \\
                \gamma_{M1}x_{M1}  & \gamma_{M2}x_{M2} & \cdots & 0
            \end{matrix} &
            \begin{matrix}
                \gamma_{1,M+1}x_{1,M+1} \\
                \gamma_{2,M+1}x_{2,M+1} \\
                \vdots \\
                \gamma_{M,M+1}x_{M,M+1}
            \end{matrix} \\ \hline
            \begin{matrix}
                \gamma_{M{+}1,1}x_{M{+}1,1} &
                \gamma_{M{+}1,2}x_{M{+}1,2} &
                \cdots &
                \gamma_{M{+}1,M}x_{M{+}1,M}
            \end{matrix} &
            \mathbf{0}
        \end{array}\right),
    \]
    \[
        \gamma_{ij} = \left\{
            \begin{aligned}
                & \frac{1}{\lambda_i - \lambda_j},\ 1\leq i{\neq}j \leq M{+}1,\\
                & 0,\ i=j
            \end{aligned}
            \right.
    \]
    and we use the convention:
    \[
        \lambda_{M{+}1} = 0.
    \]

    The operator norm of \( \Gamma \) is:
    \[
        \|\Gamma\|_{\mathrm{op}} =
        \frac1N
        \frac{1}{\min\limits_{1\leq i{\neq}j \leq M{+}1}|\lambda_i - \lambda_j|} =
        \]
    \[
        = \frac1N
         \max\left\{
         \frac{1}{
             \min\limits_{1\leq i{\neq}j \leq M }{|\lambda_i - \lambda_j|}},
         \frac{1}{
             \min\limits_{1\leq j \leq M}{|\lambda_j|}}
         \right\}
        \]
\end{lem}

\begin{thm}
Suppose \( A \) has simple spectrum and the following inequality holds:
\[
    \left\| \mathbb{B} \right\|_{\mathrm{op}}
        \leq 
        \frac{N}{4}
         \min\left\{
             \min\limits_{1\leq i{\neq}j \leq M }{|\lambda_i - \lambda_j|},
             \min\limits_{1\leq j \leq M}{|\lambda_j|}
         \right\}.
 \]

Then the spectrum of disturbed matrix \( \mathbb{A} - \mathbb{B} \) is
\[
    \sigma\left(\mathbb{A}\right) =
        \left\{
            N\lambda_1 - x_{11}^o, \ldots, N\lambda_M - x_{MM}^o
        \right\}
    \cup \sigma_{M{+}1}.
\]

The eigenvectors
    \( \hat{f}_j,\ \hat{f}_{j,k},\ j{=}\overline{1,M},\ k{=}\overline{1,N{-1}} \)
    of the matrix \( \mathbb{A}{-}\mathbb{B} \),
    the values \( x_{jj}^o,\ j{=}\overline{1,M} \)
    and the set \( \sigma_{M{+}1} \) are in the following bounds:
\[
    \lvert x_{jj}^o\rvert,
    \ \max_{\lambda\in\sigma_{M{+}1}} \lvert\lambda\rvert
    \leq 4\|B\|,
\]
\[
    \left\| \hat{f}_j - f_j \right\|_2,
    \ \left\| \hat{f}_{j,k} - f_{j,k}\right\|_2
    \leq
    \frac4N \|B\|
         \max\left\{
         \frac{1}{
             \min\limits_{1\leq l{\neq}p \leq M }{|\lambda_l - \lambda_p|}},
         \frac{1}{
             \min\limits_{1\leq l \leq M}{|\lambda_l|}}
         \right\}
\]
for all \( j{=}\overline{1,M}, k{=}\overline{1,N-1} \).
\end{thm}

\section{Perturbed Kronecker products' spectra}

Now we consider Kronecker product
\[
    A\otimes B =
    \begin{pmatrix}
        a_{11} B & \cdots & a_{1N} B \\
        \vdots   & \ddots & \vdots \\
        a_{N1} B & \cdots & a_{NN} B
    \end{pmatrix}
    \in \mathbb{K}^{{MN}{\times}{MN}}
\]
of squared matrices
\( A={(a_{ij})}\in\mathbb{K}^{N{\times}N},
 \ B={(b_{ij})}\in\mathbb{K}^{M{\times}M}. \)
We will analyze its spectral properties
    under small-norm perturbations:
\begin{equation}\label{-kronperturb}
    A\otimes B - F.
\end{equation}

% TODO:
% We will also address special
%     perturbations of the following form (cf.~\cite{XIANG2005210}):
% \[
%     (A-\Delta A)\otimes (B - \Delta B).
% \]

Kronecker product has several appealing properties~\cite{bellman-matrices-kron}.
\begin{itemize}
\item It is associative:
    \[ A\otimes (B\otimes C) = (A\otimes B)\otimes C. \]
\item It is distributive with respect to addition:
    \[ (A+B)\otimes(C+D) = A\otimes C + A\otimes D + B\otimes C + B\otimes D. \]
\item The Kronecker product of matrix products has the following property:
    \[ (AB)\otimes(CD) = (A\otimes C)(B\otimes D) \]
    whenever products \( AB \) and \( CD \) make sense.
\item The trace of \( A\otimes B \) is \[ \operatorname{tr}(A\otimes B) = \operatorname{tr}A\operatorname{tr}B. \]
\item If \( A \) and \( B \) are both symmetric matrices,
      then \( A\otimes B \) is symmetric as well.
\end{itemize}
Note also that the ``tiled'' matrix from the last example
    can be represented as a Kronecker product:
\[
    \mathbb{A} =
    \begin{pmatrix}
    A & \cdots & A\\
    \vdots & \ddots & \vdots \\
    A & \cdots & A\end{pmatrix} =
        J_N\otimes A.
    \]

\begin{lem}
Suppose \( A \) and \( B \) have simple structure,
    i.e. \( A \) has \( N \) eigenvectors
    \( f_1, \ldots, f_N \)
    whose corresponding eigenvalues are \( \mu_1, \ldots, \mu_N \)
    and \( B \) has eigenvectors \( h_1, \ldots, h_M \)
    with the eigenvalues \( \lambda_1, \ldots, \lambda_M \).
Then \( A\otimes B \) also has simple structure;
    it has \( MN \) independent eigenvectors \( f_i\otimes h_j,\ i{=}\overline{1,N}, j{=}\overline{1,M} \)
    and the corresponding eigenvalues are \( \mu_i \lambda_j \).
\end{lem}


Now suppose that among these pairwise products \( \mu_i \lambda_j \)
    there are only \( s \) distinct values \( \nu_1, \ldots, \nu_s \).
To each eigenvalue \( \nu_k \) (\( k{=}\overline{1,s} \)) there corresponds
    an eigenspace \[ E_k = \operatorname{span}(f_i\otimes h_j;\ \mu_i\lambda_j = v_k,\ i{=}\overline{1,N},\ j{=}\overline{1,M}) \subset \mathbb{K}^{MN}. \]
These eigenspaces form a direct-sum decomposition of \( \mathbb{K}^{MN} \):
    \[ \mathbb{K}^{MN} = E_1 \oplus \cdots \oplus E_s. \]
Any vector \( x\in\mathbb{K}^{MN} \) can be uniquely represented
    in the form
    \begin{equation}\label{-decomposition-x}
        x = x_1 + \cdots + x_s,\ x_k\in E_k,\ k=\overline{1,s}.
    \end{equation}
This direct-sum decomposition of \( \mathbb{K}^{MN} \)
    corresponds to a decomposition of the identity matrix \( E\in \mathbb{K}^{MN{\times}MN} \)
    (which defines an identity operator)
    into a sum of matrices of the spectral projections:
    \[
        E = P_1 + \cdots + P_s.
    \]
The spectral projection \( \mathcal{P}_k \) (\(k{=}\overline{1,s}\)) is given by the formula
    \[
        \mathcal{P}_k x = x_k \in E_k\subset \mathbb{K}^{MN}
    \]
    with respect to the decomposition~\eqref{-decomposition-x} of \( x \).

For any matrix \( X\in \mathbb{K}^{MN{\times}MN} \)
    the following trivial equality holds:
    \[
        X = \sum_{i,j=1}^s P_i X P_j.
    \]

The matrix \( A\otimes B \) can be decomposed into
    \[
        \mathcal{A} = \sum v_j P_j.
    \]

Now we should be able to reproduce the same steps as before
    to retrieve the estimates.

% \begin{center}
% \textbf{Lemma.}
% {\it
The natural way to define \( J \) is as follows:
    \[
        JX = \sum_{j=1}^s P_j X P_j.
    \]
The system of equations
    \[\left\{\begin{aligned}
        & \mathcal{A}\Gamma X - (\Gamma X) \mathcal{A} = X - JX, \\
        & J\Gamma X = 0,\ X\in \mathbb{K}^{MN{\times}MN}
    \end{aligned}\right.\]
    has the unique solution:
    \[
        \Gamma X = \sum_{1\leq i{\neq}j \leq s} \frac{1}{\nu_i-\nu_j} P_i X P_j.
    \]
    The norm of \( \Gamma \) is:
    \[
        \|\Gamma\|_{\mathrm{op}} = \gamma = \frac{1}{\min_{1\leq i{\neq}j\leq s}\lvert\nu_i - \nu_j\rvert}
    \]
% \/}
% \end{center}


\begin{thm}
    Consider the perturbed matrix~\eqref{-kronperturb}
        \[
            A{\otimes}B - F.
        \]
    Let \( A\in\mathbb{K}^{N{\times}N} \) and \( B\in\mathbb{K}^{M{\times}M} \)
        be diagonalizable matrices.
    Let \( f_1, \ldots, f_N \) be the eigenvectors of \( A \)
        corresponding to the eigenvalues \( \mu_1, \ldots, \mu_N \)
        and let \( h_1, \ldots, h_M \) be the eigenvectors of \( B \)
        corresponding to the eigenvalues \( \lambda_1, \ldots, \lambda_M \).
    The spectrum of their Kronecker product \( A{\otimes}B \)
        is composed of all the possible pairwise products \( \mu_i \lambda_j \)
        and the corresponding eigenvectors are \( f_i\otimes h_j \).
    Suppose that out of these \( MN \) eigenvalues only \( s \) are distinct:
        \( \nu_1, \ldots \nu_s \).

    Suppose
    \[
        \|F\| \leq \frac14 \gamma^{-1} = \frac14 \min_{1\leq i{\neq}j\leq s}\lvert\nu_i - \nu_j\rvert.
    \]

    Then \( A{\otimes}B - F \) is similar to
    \[ \sum_{k=1}^s \nu_k P_k - JX^o = \sum_{k=1}^s (\nu_k P_k - P_k X^o P_k) \]
    for some \( X^o \in \mathbb{K}^{MN{\times}MN} \),
    \( \|X^o - F\|\leq 3\|F\| \).

    All the eigenvalues of \( A{\otimes}B - F \) are contained in the circles
    \[
        \Omega_k = \left\{
            \lambda\in\mathbb{C};
            \ \lvert\lambda - \nu_k\rvert \leq 4\|F\|
            \right\},
        \ k{=}\overline{1,s}.
    \]
    There is at least one eigenvalue in each of these circles.

    Suppose the eigenvalue \( \nu_k=\mu_{i_k}\lambda_{j_k} \) of \( A{\otimes}B \) has multiplicity \( 1 \),
        that is it has the only eigenvector \( v_k = f_{i_k}{\otimes}h_{j_k} \).
    It is equivalent to the statement that the eigenvalue \( \mu_{i_k} \)
        of \( A \) and the eigenvalue \( \lambda_{j_k} \) of \( B \)
        are both of multiplicity \( 1 \).
    Then \( A{\otimes}B - F \) has eigenvalue in the circle \( \Omega_k \)
        and the corresponding eigenvector \( \hat{v}_k \) is within bounds
    \[
        \|\hat{v}_k - v_k\| \leq 4\gamma \|F\|.
    \]
    If \( \nu_k \) is well separated from all the other eigenvalues of \( A{\otimes}B \):
    \[
        \min_{l\neq k}
        \lvert
        \nu_k - \nu_l
        \rvert
        \geq 4\|F\|,
    \]
    then \( \nu_k \) is the only eigenvalue of \( A{\otimes}B - F \)
    in that circle.
\end{thm}

For example, in the case of a ``tiled'' matrix
\[
    J_N{\otimes}B =
    \begin{pmatrix}
        B & \cdots & B \\
        \vdots & \ddots & \vdots \\
        B & \cdots & B
    \end{pmatrix}
\]
    we would have
    \( \nu_1=N \),
    \( \nu_2=0 \).
Let \( \lambda_1,\ldots,\lambda_M \)
    be the eigenvalues of \( B \).
Spectrum of \( J_N{\otimes}B \) is
    \[
        \sigma(J_N{\otimes}B) = \left\{ \mu_i\lambda_j;\ i{=}\overline{1,2},\ j{=}\overline{1,M}\right\} = \{0\}\cup N\sigma(B).
    \]
All of these eigenvalues except for \( 0 \)
    are of multiplicity \( 1 \)
    and well-separated for sufficiently large \( N \).
Then \( \gamma=\frac1N \).
This directly implies the theorem of the previous section.

These results might be refined
    with the use of the theorem on splitting an operator~\cite{baskakov1987theorem}
    which allows to consider each eigenvalue individually
    and obtain more precise estimates for the corresponding
    eigenvalue of the perturbed matrix.
% The higher the gap between a picked eigenvalue
%     and the rest of the spectrum
%     the higher the precision of the estimate
%     for the corresponding eigenvalue of the perturbed matrix.
    

\section{Conclusion}

We have derived bounds
    for perturbations of Kronecker products
    of matrices using the method of similar operators
    which is a powerful tool in perturbation theory.
Developed in the context of Banach spaces
    this method turns out to be just as useful
    in finite-dimensional problems.

The problem of perturbation analysis for the Kronecker products
    despite being old still has a whole open field for future research.
An example of more general yet quite similar problem
    is that of an analysis of tensor products
    of finite-dimensional operators (defined by matrices)
    and abstract operators in Banach spaces.

\appendix
\section{Definitions}


\section{References}
\nocite{*}
\printbibliography
\end{document}