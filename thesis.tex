%\documentclass[a4paper]{extarticle}
\documentclass[14pt,a4paper]{extarticle}


% \usepackage[left=30mm,right=15mm,top=25mm,bottom=20mm]{geometry}
\usepackage{marginnote}
\usepackage{hyperref}
\usepackage{tabularx}
% ----
\usepackage[russian,english]{babel}
\usepackage{fontspec}
\setmainfont{CMU Serif}
% \setmainfont[SizeFeatures={Size=14}]{CMU Serif}


\usepackage{amsmath,amsthm,amssymb,euscript,calc,graphicx,ifthen,textcomp,rotating,cmap,xstring,etoolbox,xparse,l3regex,xargs,changepage,tocloft,titlecaps,enumitem,upgreek}
\usepackage{mathrsfs}


 \newtheorem{thm}{Theorem}
 \newtheorem{thme}{Theorem}
 \newtheorem{lem}{Lemma}
 \newtheorem{crl}{Corollary}
 \newtheorem{prop}{Proposition}
 \newtheorem{stm}{Statement}
 \newtheorem{dfn}{Definition}
 \theoremstyle{definition}
 \newtheorem{exm}{Example}
 \newtheorem{remn}{Remn}
 \newtheorem*{rem}{Remark}
 \newtheorem*{reme}{Remark}


\usepackage{biblatex}
\addbibresource{thesis.bib}
% \renewcommand{\baselinestretch}{1.3}

\begin{document}

\include{inc_review}
\newpage

\begin{center}
  \textbf{ВОРОНЕЖСКИЙ ГОСУДАРССТВЕННЫЙ УНИВЕРСИТЕТ}
\end{center}

\begin{flushright}
  На правах рукописи?
\end{flushright}

\vfill

\begin{center}
  \textsc{\Large
    Козлуков\\
    Сергей Викторович
  }
  \\[1.5cm]
  \textbf{SPECTRAL PROPERTIES OF CERTAIN PERTURBED BLOCK MATRICES}\\[.5cm]
  {\Huge DRAFT VERSION}\\[1.5cm]
  01.01.01 -- вещественный, комплексный и функциональный анализ?\\[1.5cm]
  \textbf{БАКАЛАВРСКАЯ РАБОТА}
\end{center}

\vfill
\begin{flushright}
  Научный руководитель\\
  доктор физико-математических наук\\
  профессор Баскаков~А.~Г.
\end{flushright}
 
\vfill
\begin{center}
  Воронеж~--~2018
\end{center}

\pagestyle{empty}
\newpage

\setcounter{tocdepth}{2}
\tableofcontents
\newpage

% \begin{abstract}
% \end{abstract}

\section*{Нотация}

В работе используются следующие обозначения и соглашения

\( f: X\to Y: x\mapsto f(x) \) --- функция \( f \)
из domain множества \( X \) в target множество \( Y \),
сопоставляющая каждому \( x\in X \) некоторый \( y=f(x) \in Y\);

\( \mathbb{N} = \{ 1, 2, \ldots \}\) --- множество натуральных чисел без нуля;

\( \mathbb{Z} \) --- кольцо целых чисел;

\( \mathbb{R} \) --- поле вещественных чисел.

Промежутки вещественной прямой обозначаются пределами, разделёнными двумя
точками так, чтобы отличать интервалы  ---
\( (a..b] \subset \mathbb{R} \) --- от упорядоченных пар: \( (a, b)\in\mathbb{R}^2 \).

\( [a..b] = \{ x\in\mathbb{R}:\ a\leq x\leq b\} \) --- отрезок
от \( a \) до \( b \), \( a\leq b \);

\( [a..b) = \{ x\in\mathbb{R}:\ a\leq x < b\} \) --- полуинтервал;

\( (a..b) = \{ x\in\mathbb{R}:\ a < x < b\} \) --- открытый интервал;

\( \mathbb{R}_+ = [0,+\infty) \) --- множество неотрицательных вещественных чисел;

\( \mathbb{C} \) --- поле комплексных чисел;

\( \mathbb{K} \) --- одно из полей: \( \mathbb{R} \) или \( \mathbb{C} \);

\( M{\times}N = \{ (m, n):\ m{\in}M,\ n{\in}N \} \), где \( M \) и \( N \)
--- множества, обозначает прямое произведение этих множеств;

\( I \) означает тождественный оператор, а \( E \) единичную матрицу;

\( \sigma(A) \) --- спектр матрицы или оператора \( A \);

\( \rho(A) \) --- резольвентное множество матрицы (оператора) \( A \);

\( \mathtt{Hom}(\mathscr{X}, \mathscr{Y})\) --- Банахово пространство ограниченных
линейных операторов, определённых на \( \mathscr{X} \) с значениями в \( \mathscr{Y} \).

\( \mathtt{End}\mathscr{X} \) --- Банахова алгебра операторов на
\( \mathscr{X} \);

\newpage

\section*{Интродукция}
Работа посвящена исследованию спектральных свойств определённого класса, или,
вернее, классов матриц,
особенно специальных матриц, возникающих в отношении теории возмущения
графов~\cite{cvetkovic1997eigenspaces}.
Основным инструментом исследования является метод подобных операторов,
развиваемый~\cite{baskakov1986theorem,baskakov1987theorem,baskakov1994spectral,baskakov2002splitting}
А.~Г.~Баскаковым и его командой. Именно, используется представление матрицы
в виде разности хорошо изученной части и пренебрежимо-малого возмущения.
Так, целью работы является локализация собственных значений и собственных векторов
матриц специального вида.


В частности, мы рассматриваем матрицу смежностей
``почти-полного орграфа''~\cite{Koz17,sergekozlukov@vspu},
все элементы которых за небольшим числом --- единицы,
рассматриваем возмущения аналогичной матрицы, составленной из одинаковых
квадратных блоков --- здесь мы её называем``tiled
matrix''~\cite{Koz18,sergekozlukov@currentproblems},
и, наконец, возмущения кронекеровых произведений квадратных матриц
~\cite{Koz18,sergekozlukov@currentproblems,bellman-matrices-kron,XIANG2005210}.
Заметим, что последние возникают в связи с некоторыми операциями на графах~\cite{cvetkovic1997eigenspaces}.

\section{Элементы теории графов}
Под графом мы понимаем тройку \((V, E, \phi)\), состояющую
из множества \( V \) вершин, множества \( E \) рёбер,
и отображения \( \phi: E\to V^2\). Где не означено иное,
множество \( V \) будем считать конечным и пронумерованным: \( V = \{ v_1, v_2,
\ldots, v_n \}\). 
Будем говорить, что ребро \( e \) \emph{исходит из} (\emph{is originating at})
вершины \( v_i\in V \)
и входит (\emph{is terminating at}) в вершину \( v_j\in V \),
если \( \phi(e)=(v_i, v_j)\). При этом вершины \( v_i, v_j \) называются
\emph{смежными} по отношению друг к другу, и \emph{инцидентными} к ребру \( e \).
На практике мы будем считать кратные рёбра неразличимыми,
так что граф однозначно задаётся его \emph{матрицей смежностей}
\[ A = ( a_{ij} ) \in \mathtt{Mat}_n(\mathbb{R}), \]
\[ a_{ij} = \mathtt{card}\{ e: \phi(e)=(v_i, v_j)\}, \]
где \( \mathtt{Mat}_n(\mathbb{R}) \) --- Банахова алгебра всех
вещественно-значных матриц размера
\( n{\times}n \) с естественной операцией умножения.
Значение \( \lambda \in \mathbb{C} \) называют собственным значением
матрицы \(
A\in\mathtt{Mat}_n(\mathbb{R}) \) если матрица \( A - \lambda E \) с \( E \) --- единичной
матрицей, необратима. Множество всех таких значений называется
\emph{спектром} матрицы \( A \) и обозначается \( \sigma(A) \).
При фиксированном \( \lambda \),  множество всех векторов \( h\in\mathbb{C}^n \)
для которых \( A h = \lambda h \) называется собственым подпространством матрицы
\( A \), соответствующим собственному значению \( \lambda \).
Спектры и собственные подпространства матриц смежностей несут в себе важную
информацию о соответствующих графах. Однако, точное вычисление собственных
значений и векторов на практике чрезвычайно ресурсоёмкая задача, страдающая
своего рода \emph{curse of dimensionality}. Поэтому требуются дешёвые и
достаточно точные оценки.

Различные спектры графов определяются с помощью различных матриц,
соответствующих этим графам.
Так называемый \emph{simple spectrum} --- простой спектр --- графа
определяется как спектр матрицы смежностей.
В прикладных моделях часто требуется рассматривать комбинации
    матриц \( A \),
    \( D \) (``out-degree'' матрица, диагональные элементы которой
    содержат исходящие степени вершин),
    \( E \) (единичная матрица)
    и матрицы \( \mathcal{J}_N \) (матрица единиц).
Такие комбинации естественным образом возникают во многих стохастических
моделях~\cite[p.~184]{cvetkovic2010introduction}.
Спектральные свойства таких матриц
часто играют существенную роль в этих моделях.
Например, Марковское случайное блуждание на графе
    приводит к понятию т.н. ``eigenvector centrality''
    в сети~\cite{ilprints422,bonacich1972factoring}.
Eigenvector-ранг \( i \)-й вершины определяется как
\( i \)-я координата
доминирующего (т.е. соответствующего наибольшему по модулю собственному
значению) (левого) собственного вектора 
матрицы переходов рассматриваемого процесса.
Этот собственный вектор
    задаёт (единственное) стационарное распределение этого процесса.
Алгоритм PageRank~\cite{ilprints422}
    изначально лежавший в основе поискового движка Google
    использует метод power-итерации для вычисления главного собственного
    вектора.
Скорость сходимости метода зависит от
    отношения двух наибольших сосбтвенных значений.
Stability of a stationary distribution
    is determined~\cite{meyer1994sensitivity}
    by the condition number
    which is bounded from below
    by the spectral gap --- the distance between
    the two largest eigenvalues
    of the transition matrix of a process.
The method of estimation of almost-invariant sets
    proposed in~\cite{schwartz2006fluctuation}
    also relies on spectral decompositions of such matrices.
In the Susceptible-Infective-Susceptible model
    a viral spread in a network
    is modeled~\cite{wang2003epidemic,chakrabarti2008epidemic} as a Markov process
    with \( 2^N \) states.
It is a discrete-time model of
распространения вируса в~сети, в~которой спектральный радиус матрицы смежностей
графа сети  оказывается пороговым значением \( ^1/_{\tau_0} \) отношения
\( ^1/_\tau = ^\delta/_\nu \) интенсивности~\( \delta \) исцеления
инфецированных узлов и~интенсивности~\( \nu \) заражения узлов, смежных
инфецированным. 
An asymptotic (endemic or epidemic) behaviour of such a system
    is determined by \(^1/\tau\) being smaller or greater than
    spectral radius (the largest absolute eigenvalue)
    of the adjacency matrix.

One of the most generic and common operations on graph --- 
    \emph{non-complete extended p-sum}s~(NEPS) of
    graphs~\cite[p.~44]{cvetkovic2010introduction}~\cite{cvetkovic1997eigenspaces}
    --- is defined in terms of Kronecker products of graphs.

For more details and comprehensive description
    of the graph spectra theory
    and its applications
    refer to~\cite{cvetkovic1980spectra,cvetkovic2010introduction,godsil2013algebraic}.


\section{Introduction to the method of similar operators}

In some applications
    it is infeasible to compute the exact eigenvalues.
Then one needs to make reasonable estimates.
One of the methods of such estimations is the method of simiar operators
originated in the works of Friedrichs~\cite{friedrichs1965advanced} and being developed in
the abstract setting by
Baskakov~\cite{baskakov1986theorem,baskakov1987theorem,baskakov1994spectral,baskakov2002splitting}.
It relies on contraction mappings in Banach spaces
    and the Banach fixed-point theorem.
This approach is often superior to usual methods of perturbation analysis
    that use series expansions.
We are only concerned with finite-dimensional problems
    so we will only state the required notation and theorems
    in a simplified form.

Let \( \mathbb{K}\in \{ \mathbb{R}, \mathbb{C} \} \)
    be a field of either real or complex numbers.
We consider the vector space \( \mathbb{K}^n,\ n\in \mathbb{N} \)
    supplied with Euclidean structure:
    \[
        (x, y){=}\sum_{k=1}^n x_k\overline{y_k},
        \ x{=}(x_1,\ldots, x_n),
        \ y=(y_1,\ldots, y_n)
        \in \mathbb{K}^n
        \]
    and the \( \mathrm{L}_2 \)-norm:
    \(
        \|x\|_2^2{=}(x,x).
        \)
We also consider the canonical basis \( e_1, \ldots, e_n \)
    in \( \mathbb{K}^n \) given by
    \( {(e_i)}_j = \delta_{ij},\ i,j=\overline{1,n} \)
    (\(\delta_{ij} \) is the Kronecker symbol).
When \( V_1, V_2 \) are normed vector spaces
    we denote by \( \mathtt{Hom}(V_1, V_2) \)
    the space of bounded linear mappings
    from \( V_1 \) to \( V_2 \).
An algebra of bounded linear endomorphisms
    from a Banach space \( V \)
    into itself
    is denoted by \( \mathtt{Hom}(V) = \mathbb{Hom}(V, V) \).
It is a Banach algebra with the operator norm:
    \[
        \|A\|_{\mathrm{op}} =
        \sup_{
            \substack{\|x\|=1,\\ x\in V}
        } \|A x\|,\ A\in \mathtt{End}(V).
        \]
Together with \( \mathtt{Hom}(\mathbb{K}^n, \mathbb{K}^m) \)
    we consider its isomorphic space \( \mathtt{Mat}_{m{\times}n}(\mathbb{K}) \)
    of matrices of the size \( m{\times}n \)
    with entries from the field \( \mathbb{K} \).
The space \( \mathtt{Mat}_{n{\times}n}(\mathbb{K})\sim \mathtt{End}(\mathbb{K}^n) \)
    forms a Banach algebra
    when supplied with a submultiplicative norm
    \( \|\cdot\| \),
    e.g.: \( \|A\|_{\mathrm{op}} = \sup_{\|x\|_2=1,\ x\in \mathbb{K}^n} \|A x\|_2,\ \)
    \( \|A\|_{\mathrm{F}} = \sqrt{\sum_{i,j} |a_{ij}|^2},\ \)
    for 
    \( A{=}(a_{ij})\in\mathbb{K}^{n\times n} \).
Finally we will also be dealing with the isomorphic spaces
    \( \mathtt{End}(\mathtt{End}(\mathbb{K}^n)) \) and \( \mathtt{End}(\mathtt{Mat}_{n{\times}n}(\mathbb{K})) \)
    with the operator norm.
We will follow Krein
    and refer to elements of \( \mathtt{End}(\mathtt{Mat}_{n{\times}n}(\mathbb{K})) \)
    as ``transformers''.

The spectrum of a matrix \( A \)
    (the set of its eigenvalues)
    will be denoted as \( \sigma(A) \).
We call two matrices \( A_1, A_2 \) \emph{similar}
    if there is an invertible matrix \( U \)
    (the similarity matrix)
    such that \( A_1 U = U A_2 \).
Similar matrices share some spectral properties:
    they are isospectral (\( \sigma(A_1) = \sigma(A_2) \))
    and \( U \) maps the eigenvectors of one to another's:
    \( A_2 x = \lambda x \implies A_1 U x = \lambda U x \).

The most important notion
    in the abstract method of similar operators
    is that of an \emph{admissible triple}.
For our specific purposes it suffices to say
    that \( (\mathtt{Mat}_{n{\times}n}(\mathbb{K}), J, \Gamma) \)
    forms an \emph{admissible triple}
    for a matrix \( A\in\mathtt{Mat}_{n{\times}n}(\mathbb{K}) \)
    if the following conditions are met:
\begin{itemize}
    \item \( J, \Gamma \in \mathtt{End}(\mathtt{Mat}_{n{\times}n}(\mathbb{K})) \)
        are transformers;
    \item \( J \) is a projection (\( J^2 = J \));
    \item  \( \Gamma \) satisfies the equations:
        \[
            A \Gamma X - (\Gamma X) A = X - JX,
        \]
        \[
            J\Gamma X = 0,\ X\in\mathtt{Mat}_{n{\times}n}(\mathbb{K}).
        \]
\end{itemize}

The main theorem of the method
    may be now formulated as follows:

\begin{thm}
    Consider a matrix \( A - B \)
        with \( A, B \in \mathtt{Mat}_{n{\times}n}(\mathbb{K}) \).
    Suppose \( (\mathtt{Mat}_{n{\times}n}(\mathbb{K}), J, \Gamma) \)
        is an admissible triple for the matrix \( A \)
        and suppose the following inequality holds:
        \[
            \|B\|\|\Gamma\|_{\mathrm{op}} \leq \frac14.
        \]

    Then there exists such a matrix \( X^o\in\mathtt{Mat}_{n{\times}n}(\mathbb{K}) \)
        that \( A - B \) is similar to \( A - J X^o \);
        the similarity matrix is \( E + \Gamma X^o \);
        the following estimates are valid:
        \[
            \|X^o - B\| \leq 3 \left\|B\right\|,
        \]
        \[
            \operatorname{spr}(X^o) \leq \|X^o\| \leq 4 \left\|B\right\|,
        \]
        where \( \operatorname{spr}(X^o) \)
        is the spectral radius of \( X^o \) (the largest absolute eigenvalue).
    Such \( X^o \) can be found as the limit of a convergent sequence
        \( \left( \Phi^k(0);\ k\in\mathbb{N} \right) \)
        in a Banach algebra \( \mathtt{Mat}_{n{\times}n}(\mathbb{K}) \).
        Here \( \Phi \) is a nonlinear contraction mapping
        defined on the ball \( \{X\in\mathtt{Mat}_{m{\times}n}(\mathbb{K});\ \|X-B\|\leq 3\|B\| \} \)
        and given by
    \[
        \Phi(X) = B\Gamma X - (\Gamma X)J(B + B\Gamma X) + B
    \]
        and \( \Phi^k = \underbrace{\Phi\circ\cdots\circ\Phi}_{k\ \text{copies}} \)
        denotes the composition.
\end{thm}

\section{Almost complete graph adjacency matrix}
\sloppy
Consider a digraph defined by the following adjacency matrix:
\[
    A = \mathcal{J}_N - B = \begin{pmatrix}1 & \cdots & 1 \\ \vdots & \ddots & \vdots \\ 1 & \cdots & 1\end{pmatrix} - B,
\]

Here \( \mathcal{J}_N \) is the all-ones matrix.
The unity on the intersection
of \( i \)'th row and \( j \)'th column of \( B \)
corresponds to an edge from \( i \) to \( j \)
being absent in the graph.
This example has already been considered in~\cite{sergekozlukov@volgograd}
and here we will reproduce some steps to demonstrate the technique.

Рассмотрим матрицу \( A \) размера \( N\times N \),
 составленную из \( M \) нулей и \( N^2 - M \) единиц.
\[
    A = \mathcal{J}_N - B = \begin{pmatrix}1 & \cdots & 1 \\ \vdots & \ddots & \vdots \\ 1 & \cdots & 1\end{pmatrix} - B,
\]
Здесь \( \mathcal{J}_N \)~--- матрица, составленная из \( N\times N \) единиц,
 а~\( B \) имеет единицы в~точности на тех \( M \) местах,
 где в~\( A \) стоят нули.

Как матрица смежности, \( A \) соответствует орграфу,
 полученному из полного графа с~петлями на \( N \) вершинах
 удалением некоторых \( M \) из \( N^2 \) р\"ебер.
Некоторые важные свойства графа связаны с~спектром его матрицы смежностей.
Спектральная теория графов и~е\"е приложения подробно рассмотрены
в~монографии~\cite{cvetkovic1997eigenspaces,cvetkovic1980spectra,cvetkovic2010introduction,godsil2013algebraic}.

Что можно сказать о~собственных значениях матриц рассматриваемого вида?

Спектр \( \sigma\left( \mathcal{J}_N \right) \)
 матрицы \( \mathcal{J}_N \) легко считается:
 \( \mathcal{J}_N^2 = N \mathcal{J}_N, \) т.е.
 \( \lambda(\lambda - N) \)~--- аннулирующий и, что легко проверить,
 минимальный многочлен матрицы \( \mathcal{J}_N \), а~значит
 \( \sigma\left( \mathcal{J}_N \right) = \left\{ 0,N \right\}. \)

One can easily find the minimal annihilating polynomial of \( \mathcal{J}_N \)
    to be \( \lambda(\lambda - N) \).
This comes from the fact that \( \mathcal{J}_N^2 = N \mathcal{J}_N \).
Consequently the spectrum of \( \mathcal{J}_N \) is
\[
    \sigma(\mathcal{J}_N) = \left\{0, N\right\}.
\]

При достаточно малых \( M \),
 спектры матриц \( \mathcal{J}_N \) и~\( A \) будут ``близки''.
Методом подобных операторов (см.~\cite{baskakov1986theorem,baskakov1987theorem,baskakov1994spectral,baskakov2002splitting}),
 позволяющим для возмущений ``идеального'' объекта, спектральные свойства которого известны,
 найти элемент рассматриваемой алгебры, изоспектральный возмущ\"енному,
 но имеющий более удобную для вычислений структуру,
 в~статье доказывается
\begin{thm}\label{nk:thm:almost-all-ones}
    Пусть \( M < \frac{1}{16} N^2 \),
    тогда спектр матрицы \( A \) можно представить в~виде
    объединения \( \sigma\left(A\right) = \sigma_1 \cup \sigma_2 \)
    непересекающихся
    одноэлементного множества \( \sigma_1=\{\lambda_1\} \)
    и~множества \( \sigma_2 \), удовлетворяющих условиям:
    \[ \sigma_1 \subset \left\{ \mu\in\mathbb{R}; \lvert \mu - N \rvert < 4\sqrt{M} \right\}, \]
    \[ \sigma_2 \subset \left\{ \mu\in\mathbb{C}; \lvert \mu \rvert < 4\sqrt{M} \right\}. \]
\end{thm}

Доказательство состоит в~построении уравнения для матрицы, подобной \( A \),
 но устроеной ``проще''. Решение возникающего нелинейного уравнения
 в~Банаховой алгебре \( \mathtt{Mat}_N\mathbb{C} \)
 доставляется методом простых итераций (см., например,~\cite{baskakov1986theorem,baskakov1987theorem,baskakov1994spectral,baskakov2002splitting}).

Подобие матриц \( \mathcal{A}_1, \mathcal{A}_2 \)
 понимается в~смысле существования обратимой матрицы \( \mathcal{U} \),
 такой что \( \mathcal{A}_1 \mathcal{U} = \mathcal{U} \mathcal{A}_2 \).
Подобные матрицы изоспектральны (их спектры совпадают).

Провед\"ем предварительные преобразования.

\begin{lem}
    Матрица единиц 
    \( \mathcal{J}_N =
    \begin{pmatrix}
        1 & \cdots & 1 \\
        \vdots & \ddots & \vdots \\ 
    1 & \cdots & 1 \end{pmatrix} \),
    подобна матрице
    \[
        \mathcal{A} = \begin{pmatrix}
            N & 0 & \cdots & 0 \\
            0 & 0 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & 0 \end{pmatrix}. \]
    Точнее, существует ортогональная матрица \( \mathcal{U} \),
    такая что
    \( \mathcal{J}_N = \mathcal{U}\mathcal{A} \mathcal{U}^{-1} \).
\end{lem}
\begin{proof}
    Собственному значению \( 0 \) соответствует \( N-1 \) независимый собственный вектор
        \( f_1 = {\left(1,-1,0,\ldots,0\right)}, \ldots,
           f_{N-1} = {\left(0,\ldots,0,1,-1\right)} \),
    а~собственному значению \( N \) матрицы \( \mathcal{J}_N \) 
    соответствует собственный вектор \( f_N = {\left(1,\ldots,1\right)} \).
    Применив ортогонализацию Грама-Шмидта, получим ортонормальную систему \( h_1, \ldots, h_N \):
    \[
        h_k = \frac{1}{\sqrt{k(k+1)}}
            \left(\smash{\underbrace{1,~\ldots,~1,}_{k \text{ раз}}}~-k,~0,~\ldots,~0\right)
            \in \mathbb{R}^N, \quad k={1, \ldots, N-1} \]
    \[
        h_N = {\left(1,~\ldots,~1\right)} \in \mathbb{R}^N, \]

The only non-zero eigenvalue \( N \) has the corresponding eigenvector
\[
    h_N = \frac{1}{\sqrt{N}} \left(1, \ldots, 1\right)\in\mathbb{R}^N.
\]
The null-space of \( \mathcal{J}_N \) is orthogonal to \( h_N \)
    and allows the orthonormal basis:
\[
    h_k = \frac{1}{\sqrt{k(k+1)}} \left(\underbrace{1, \ldots, 1}_{k\ \text{copies}}, -k, 0, \ldots, 0\right),\ k=\overline{1, N-1}.
\]


    В~качестве матрицы \( \mathcal{U} \) выберем матрицу,
    имеющую столбцами векторы \( h_N, h_1, \ldots, h_{N-1} \):
    \[ \mathcal{U} =
    \begin{pmatrix}
        \frac{1}{\sqrt N} &  \frac{1}{\sqrt2} &  \frac{1}{\sqrt{6}} & \cdots & \frac{1}{\sqrt{(N-2)(N-1)}} & \frac{1}{\sqrt{(N-1)N}} \\
        \frac{1}{\sqrt N} & -\frac{1}{\sqrt2} &  \frac{1}{\sqrt{6}} & \cdots & \frac{1}{\sqrt{(N-2)(N-1)}} & \frac{1}{\sqrt{(N-1)N}} \\
        \frac{1}{\sqrt N} & 0                 & -\frac{2}{\sqrt{6}} & \cdots & \frac{1}{\sqrt{(N-2)(N-1)}} & \frac{1}{\sqrt{(N-1)N}} \\
        \frac{1}{\sqrt N} & 0                 &  0                  & \cdots & \frac{1}{\sqrt{(N-2)(N-1)}} & \frac{1}{\sqrt{(N-1)N}} \\
        \vdots            & \vdots            &  \vdots             & \ddots & \vdots                      & \vdots   \\
        \frac{1}{\sqrt N} & 0                 &  0                  & \cdots & \frac{1}{\sqrt{(N-2)(N-1)}} & \frac{1}{\sqrt{(N-1)N}} \\
        \frac{1}{\sqrt N} & 0                 &  0                  & \cdots & \frac{2-N}{\sqrt{(N-2)(N-1)}} & \frac{1}{\sqrt{(N-1)N}} \\
        \frac{1}{\sqrt N} & 0                 &  0                  & \cdots & 0                  & \frac{1-N}{\sqrt{(N-1)N}}
    \end{pmatrix}.\]
\end{proof}

We conclude that the adjacency matrix \( A = \mathcal{J}_N - B \) of this graph
    is similar to \( \mathcal{A} - \mathcal{B} \)
    where \( A \) is a block matrix (subscripts denote block sizes; in what follows throughout this subsection block sizes are the same and will be omitted):
    \[
        \mathcal{A} = \left(\begin{array}{c|c}
        N & \mathbf{0}_{1{\times}(N{-}1)} \\ \hline
            \mathbf{0}_{(N{-}1){\times}1} & \mathbf{0}_{(N{-}1){\times}(N{-}1)}
        \end{array}\right) \in \mathtt{Mat}_{N{\times}N}(\mathbb{R})
    \]
    and \( \mathcal{B} \) is obtained with a similarity transform:
    \(
        \mathcal{B} = U^{-1} B U \in \mathtt{Mat}_{N{\times}N}(\mathbb{R}).
    \)
The similarity matrix \( U \) is given by stacking the eigenvectors in columns:
    \[
        U = \operatorname{columns}(h_N, h_1, \ldots, h_{N-1}) =
        \begin{pmatrix}
            \vline & \vline &        & \vline \\
            h_N    & h_1    & \ldots & h_{N-1} \\
            \vline & \vline &        & \vline

        \end{pmatrix}.
    \]


Таким образом, исходная матрица \( \mathcal{A} \) подобна матрице
\( \mathcal{A} - \mathcal{B} \), где \( \mathcal{B} = \mathcal{U}^{-1} B \mathcal{U} \).
Далее ортогональность матрицы \( U \) будет играть важную роль.

Матрицы из \( \mathtt{Mat}_N\mathbb{C} \) будем записывать в~блочном виде
\( X \sim
    \begin{pmatrix}
    x_{11} & X_{12} \\
    X_{21} & X_{22}
    \end{pmatrix}, \)
    где \( x_{11} \)~--- число,
    \( X_{12} \)~--- строка, \( X_{21} \)~--- столбец,
    \( X_{22} \)~--- квадратный блок размерности \( N-1 \).
Такие блочные матрицы сами образуют алгебру, изоморфную исходной
и~их можно естественным образом умножать
на элементы пространства \( \mathbb{C}\times\mathbb{C}^{N-1} \),
изоморфного~\( \mathbb{C}^N \):
\[
    \begin{pmatrix}
    x_{11} & X_{12} \\
    X_{21} & X_{22}
    \end{pmatrix}
    \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}
  = \begin{pmatrix}
      x_{11} x_1 + X_{12} x_2 \\
      X_{21} x_1 + X_{22} x_2
      \end{pmatrix},\quad x \sim \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}\in \mathbb{C}\times\mathbb{C}^{N-1}.
    \]
В~дальнейших выкладках изоморфные объекты понимаются взаимозаменяемыми.

Следуя общей схеме метода подобных операторов~\cite{baskakov1986theorem,baskakov1987theorem,baskakov1994spectral,baskakov2002splitting},
будем искать более ``простую'' матрицу, подобную \( \mathcal{A} - \mathcal{B} \),
в~виде \( \mathcal{A} - J X \)
с~матрицей преобразования подобия \( E + \Gamma X \),
где \( E\in{\mathtt{Mat}_N\mathbb{C}} \)~--- единичная матрица,
\( J,\Gamma : \mathtt{Mat}_N\mathbb{C}\to\mathtt{Mat}_N\mathbb{C} \)~--- линейные операторы,
действующие на алгебре \( \mathtt{Mat}_N\mathbb{C} \), подбираемые
в~ходе решения,
      прич\"ем \( J \) --- проектор (\(J^2=J\)),
      ``упрощающий'' возмущение \( JX \),
      а \( \Gamma \)
      при всех \( X\in {\mathtt{Mat}_N\mathbb{C}} \) % \( X\sim \begin{pmatrix}x_{11} & X_{12} \\ X_{21} & X_{22}\end{pmatrix} \in {\mathtt{Mat}_N\mathbb{C}} \)
      удовлетворяет уравнению
          \( \mathcal{A}\Gamma X - (\Gamma X) \mathcal{A} = X - JX. \)

% Короче, будем решать в~Банаховой алгебре матриц порядка \( N \) уравнение
% \begin{equation}\label{nk:eq:similarity}
%     (\mathcal{A-B})(E+\Gamma X) = (E+\Gamma X)(\mathcal{A} - J X), \quad X\in\mathtt{Mat}_N\mathbb{C}.
%     \end{equation}
% Оператор \( J \) обычно выбирают проектором (\(J^2=J\)).
% \( \Gamma \) определяют поточечно, как решение уравнения
% \( \mathcal{A}\Gamma X - (\Gamma X) \mathcal{A} = X - J X, \quad X\in\mathtt{Mat}_N\mathbb{C} \),
% где \( \mathcal{A}\Gamma X - (\Gamma X) \mathcal{A} = \mathtt{ad}_{\mathcal{A}} \Gamma X \),
% \(  \mathtt{ad}_{\mathcal{A}}: \mathtt{Mat}_N\mathbb{C}\to\mathtt{Mat}_N\mathbb{C} \)
% --- оператор коммутирования с \( \mathcal{A} \).
% Ясно, \( \mathcal{A}-JX \) имеет тем более простую структуру,
% чем шире ядро проектора \( J \).
% Уравнение для \( \Gamma \) в~свою очередь не позволяет сузить ядро слишком сильно.

\begin{lem}
    Операторы \( J \) и \( \Gamma \)
    следует задать формулами
    \[
        J X = \begin{pmatrix} x_{11} & 0 \\ 0 & X_{22} \end{pmatrix}, \]
    \[
        \Gamma X = \frac{1}{N} \begin{pmatrix} 0 & X_{12} \\ -X_{21} & 0 \end{pmatrix}, \]
        для \( X\sim \begin{pmatrix}x_{11} & X_{12} \\ X_{21} & X_{22}\end{pmatrix} \in \mathtt{Mat}_N\mathbb{C} \).

\end{lem}
\begin{crl}
    Спектр блочно-диагональной матрицы
    \( \mathcal{A} - JX = \begin{pmatrix} N - x_{11} & 0 \\ 0 & X_{22} \end{pmatrix} \)
    есть объединение спектров е\"е диагональных блоков:
    \[
        \sigma(\mathcal{A} - J X) = \{ N - x_{11} \} \cup \sigma(X_{22}). \]
\end{crl}
\begin{proof}
Пусть \( \Gamma \) действует по формуле
\( \Gamma X = \begin{pmatrix} \Gamma_{11}(X) & \Gamma_{12}(X) \\
                              \Gamma_{21}(X) & \Gamma_{22}(X)
                              \end{pmatrix} \), тогда
\[
    \mathcal{A} \Gamma X - (\Gamma X)\mathcal{A} = 
    \begin{pmatrix} 0 & N\Gamma_{12}(X) \\
        - N\Gamma_{21}(X) & 0
        \end{pmatrix}, \]
и~уравнение для \( \Gamma X \) сводится~к
\[
    X - J X =
    N \begin{pmatrix} 0 & \Gamma_{12}(X) \\
        - \Gamma_{21}(X) & 0
        \end{pmatrix}. \]

Значит, \( J \) может обнулить в
    \( X \sim
    \begin{pmatrix}
    x_{11} & X_{12} \\
    X_{21} & X_{22}
    \end{pmatrix} \in \mathtt{Mat}_N\mathbb{C} \)
    вс\"е, кроме двух диагональных блоков \( x_{11} \) и \( X_{22} \),
    поэтому положим
\[
    J X = \begin{pmatrix} x_{11} & 0 \\ 0 & X_{22} \end{pmatrix}, \]
\[
    \Gamma X = \frac{1}{N}\begin{pmatrix} 0 & X_{12} \\ -X_{21} & 0 \end{pmatrix}. \]
\end{proof}

Following the general method,
    we should first construct an admissible triple.
Since \( \mathcal{A} \) is block-diagonal
    it is natural to set \( J \) with the formula
    \[
        JX =
        \left(\begin{array}{c|c}
            x_{11} & \mathbf{0} \\ \hline
            \mathbf{0} & X_{22}
        \end{array}\right)
    \]
    for all block matrices
    \[
        X =
        \left(\begin{array}{c|c}
            x_{11} & X_{12} \\ \hline
            X_{21} & X_{22}
        \end{array}\right)\in\mathtt{Mat}_{N{\times}N}(\mathbb{K}).
    \]
Then \( A - JX \) is block-diagonal for any \( X \)
    and its spectrum is the union
    of its diagonal blocks' spectra:
    \( \sigma(A - JX) = \{N, \sigma(-X_{22}) \} \).

Now we can find the corresponding \( \Gamma \).
Suppose it is defined by the formula
    \[ \Gamma X = \begin{pmatrix}
        \Gamma_{11}(X) & \Gamma_{12}(X) \\
        \Gamma_{21}(X) & \Gamma_{22}(X)
        \end{pmatrix}.
    \]
Then the equations
    \[
        \mathcal{A} \Gamma X - (\Gamma X)\mathcal{A} =
        N
        \begin{pmatrix}
          0 & \Gamma_{12}(X) \\
          -\Gamma_{21}(X) & 0
        \end{pmatrix} = X - JX,
     \]
and \( J\Gamma X = 0 \) yield the result
    \[
        \Gamma X = \frac{1}{N} \begin{pmatrix} 0 & X_{12} \\ -X_{21} & 0 \end{pmatrix},\ X\in\mathtt{Mat}_{N{\times}N}(\mathbb{K}).
    \]

Теперь выпишем уравнение подобия матриц \( \mathcal{A} - \mathcal{B} \)
и \( \mathcal{A} - J X \):
\begin{equation}\label{nk:eq:similarity}
    (\mathcal{A-B})(E+\Gamma X) = (E+\Gamma X)(\mathcal{A} - J X), \quad X\in\mathtt{Mat}_N\mathbb{C}.
\end{equation}
\begin{lem}
    Уравнение~\eqref{nk:eq:similarity} эквивалентно уравнению
    \begin{equation}\label{nk:eq:fixptn}
        X = \mathcal{B} \Gamma X + \mathcal{B} - (\Gamma X)(J(\mathcal{B} (E + \Gamma X))), \quad X\in\mathtt{Mat}_N\mathbb{C}.
    \end{equation}
\end{lem}
\begin{proof}
Раскрывая скобки, уравнение~\eqref{nk:eq:similarity} можно преобразовать к виду
\begin{equation}\label{nk:eq:fixptn-ini}
    X = \mathcal{B} \Gamma X + \mathcal{B} - (\Gamma X) J X.
\end{equation}
Пусть для \( X \) выполнено~\eqref{nk:eq:fixptn-ini}.
Тогда, учитывая равенство \( J\left((\Gamma X)JX\right) = 0, \)
получим равенство
    \begin{equation}\label{nk:eq:jx}
        J X = J\mathcal{B} + J\left(\mathcal{B}\Gamma X\right) = J(\mathcal{B} (E + \Gamma X)).
    \end{equation}
Подставляя это выражение обратно в~\eqref{nk:eq:fixptn-ini},
    получим~\eqref{nk:eq:fixptn}.
Аналогично, применяя к обеим частям равенства~\eqref{nk:eq:fixptn} оператор \( J \)
    и учитывая, что \( J\left( (\Gamma X)J(\mathcal{B} (E + \Gamma X)) \right) = 0 \),
    получим~\eqref{nk:eq:fixptn-ini}
\end{proof}

Выражение в правой части уравнения~\eqref{nk:eq:fixptn} обозначим как
\[
    \Phi(X) = \mathcal{B} \Gamma X + \mathcal{B} - (\Gamma X)(J(\mathcal{B} (E + \Gamma X))).\]
Теперь покажем, что, при определ\"енных условиях,
возникшее нелинейное отображение \( \Phi:\mathtt{Mat}_N\mathbb{C}\to \mathtt{Mat}_N\mathbb{C} \) имеет инвариантным множеством
некоторый шар \( \Omega \subset \mathtt{Mat}_N\mathbb{C} \) с~центром в~нуле
(т.е.~\( \Phi(\Omega)\subset\Omega \)),
на котором оно является сжимающим.

Пусть в~\( \mathtt{Mat}_N\mathbb{C} \)
выбрана какая-нибудь субмультипликативная норма \( \|\cdot\| \)
(т.е.~норма, удовлетворяющая неравенству
 \( \| \mathcal{A}_1\mathcal{A}_2 \| \leq \|\mathcal{A}_1\|\|\mathcal{A}_2\| \)
 при всех \( \mathcal{A}_1, \mathcal{A}_2 \in \mathtt{Mat}_N\mathbb{C} \)).
Нам нужно найти такой радиус \( r \geq 0 \),
что при \( \|X\|,\|Y\| \leq r \) выполнялись бы неравенства \( \|\Phi(X)\| \leq r \)
и~\( \|\Phi(X) - \Phi(Y)\| < q\|X-Y\| \), \( q\in(0,1) \).
Обозначим
\( \beta = \|\mathcal{B}\| \), \( \gamma = \sup_{\|X\|=1} \|\Gamma X\| \).

\begin{lem}
    Пусть \( \gamma\beta < \frac14\),
    тогда шар
    \[
        \Omega = \left\{ X\in \mathtt{Mat}_N\mathbb{C}; \|X\| \leq r_0 \right\}, \]
    \[  0 < r_0 = \frac{1 - 2\gamma\beta - \sqrt{1-4\gamma\beta}}{2\gamma^2\beta} < 4\beta, \]
    удовлетворяет условию \( \Phi(\Omega)\subset\Omega \).
\end{lem}
\begin{proof}
Очевидно неравенство
    \[ \| \Phi(X) \| \leq
     \beta \gamma^2 \|X\|^2 + 2\beta\gamma\|X\| + \beta. \]
Значит, если \( r \) удовлетворяет неравенству
    \begin{equation}\label{nk:ineq:invariance-radius}
        \beta \gamma^2 r^2 + (2\beta\gamma - 1)r + \beta \leq 0,
    \end{equation}
    то \( \|\Phi(X)\| \leq r \) при всех \( \|X\| \leq r \).
Если \( \gamma\beta \leq \frac14 \),
    то ``дискриминант'' \( \Delta = 1-4\gamma\beta \)
    соответствующего уравнения положителен и~его корни вещественны.
Из знаков коэффициентов возникшего многочлена видно, что оба корня положительны.
Следовательно, наименьший положительный \( r \),
    удовлетворяющий неравенству~\eqref{nk:ineq:invariance-radius}
    есть наименьший корень
    соответствующего уравнения:
    \[ r_0 = \frac{1 - 2\gamma\beta - \sqrt{1-4\gamma\beta}}{2\gamma^2\beta}. \]
Учитывая \( \gamma\beta<\frac14 \), имеем \( r_0 < 4\beta \).
\end{proof}

Аналогичным образом устанавливается
\begin{lem}
    Пусть \(\gamma\beta<\frac14\),
    тогда \( \Phi \)~--- сжимающее отображение:
    \[ \| \Phi(X) - \Phi(Y) \| \leq q \|X - Y\|, \quad X,Y\in\Omega \]
    \[ q = (1+2\gamma r_0) \gamma\beta \leq (1+8\gamma\beta)\gamma\beta \leq \frac34. \]
\end{lem}
\begin{proof}
    \begin{align*} \| \Phi(X) - \Phi(Y) \| = \| \mathcal{B}\Gamma (X-Y) + (\Gamma X)(\mathcal{B}\Gamma X + \mathcal{B})
     - (\Gamma Y)(\mathcal{B} \Gamma Y + \mathcal{B}) \| \leq \\
        \leq
     \beta\gamma\|X-Y\| +
     \beta \gamma^2 \|X-Y\| \|X+Y\| \leq \\
        \leq
     \beta\gamma\|X-Y\| +
      2 r_0 \beta \gamma^2 \|X-Y\|.
    \end{align*}
Здесь использовано равенство
\[ (\Gamma X) J(\mathcal{B}\Gamma X) - (\Gamma Y) J(\mathcal{B}\Gamma Y) =
  \]
 \[= \frac12\left[
        \Gamma(X-Y) J(\mathcal{B}\Gamma(X+Y))
    +   \Gamma(X+Y) J(\mathcal{B}\Gamma(X-Y))
  \right].
\]
\end{proof}

Отсюда и~из теоремы Банаха о~неподвижной точке следует:
\begin{lem}
В~шаре \[ \Omega = \left\{ X\in\mathtt{Mat}_N\mathbb{C}; \quad \|X\| \leq r_0 \right\} \]
    существует и~при том единственное решение \( X^o \) уравнения~\eqref{nk:eq:fixptn},
    являющееся пределом последовательности \( \{ \Phi^k(0); k\in\mathbb{N} \} \),
    где \( \Phi^k = \Phi\circ\Phi^{k-1} \)~--- композиция.
\end{lem}

\begin{crl}
Матрица \( \mathcal{A} - \mathcal{B} \) подобна блочно-диагональной матрице \( \mathcal{A} - J X^o \):
\[ \mathcal{A} - \mathcal{B} \sim
\begin{pmatrix}
N - x_{11}^o & 0 \\
0 & -X_{22}^o
\end{pmatrix}, \]
при этом выполняются условия:
\[ \sigma\left(\mathcal{A} - \mathcal{B}\right) = \left\{N-x_{11}^o\right\}\cup \sigma\left(-X_{22}^o\right), \]
    \[ x_{11}^o\in\mathbb{R}, \lvert x_{11}^o \rvert \leq r_0 \leq 4\beta, \]
\[ \sigma\left(-X_{22}^o\right) \subset \{ \mu\in\mathbb{C}; \lvert x \rvert \leq r_0 \leq 4\beta \}. \]
\end{crl}
\begin{proof}
    Матрица \( \mathcal{A} - \mathcal{B} \) подобна блочно-диагональной \( \mathcal{A} - J X^o \),
    поэтому их спектры совпадают.
    Спектр матрицы \( \mathcal{A} - J X^o \) есть объединение спектров е\"е диагональных блоков.
    В~виду субмультипликативности нормы имеют место неравенства
    \[ \mathtt{spr}(X^o) = \max_{\lambda\in\sigma(X^o)}\lvert\lambda\rvert \leq \|X^o\| \leq r_0. \]
    Кроме того, собственное значение \( x_{11}^o \) является вещественным, как предел сходящейся вещественной последовательности.
\end{proof}

The last step is to estimate the norms and apply the theorem.
One can easily check that \( \|\Gamma\|_{\mathrm{op}} = \frac1N \).
It is also apparent that \( \|\mathcal{B}\|_{\mathrm{op}} = \|B\|_{\mathrm{op}} \leq \|B\|_{\mathrm{F}} \)
    since multiplication by the orthogonal matrix \( U \)
    is an isometry in Euclidean space.
The Frobenius norm \( {\|B\|_{\mathrm{F}} = \sqrt{\sum_{ij} b_{ij}^2} = M} \)
    of \( B \)
    reduces to the square root of the number of absent edges.
This directly implies the following

Верн\"емся, наконец, к~непосредственному доказательству основной теоремы:
\begin{proof}[Доказательство Теоремы~\ref{nk:thm:almost-all-ones}]
    Для доказательства осталось лишь выбрать подходящую субмультипликативную норму.
    Заметим, что матрица \( \mathcal{U} \),
    приводящая \( \mathcal{J}_N \) к~диагональному виду
    является ортогональной,
    поэтому умножение на \( \mathcal{U} \) или \(\mathcal{U}^{-1}\)
    есть изометрия.
    Следовательно, \( \|\mathcal{B}\|=\|B\| \).
    Рассмотрим в~пространстве \( \mathtt{Mat}_{N}\mathbb{C} \)
    норму Фробениуса \( {\left\|\cdot\right\|}_{F} \),
    определ\"енную формулой
    \( {\left\|X\right\|}_{F} = \sqrt{\sum_{ij} \lvert x_{ij}\rvert^2}, \)
    \( X = (x_{ij})\in\mathtt{Mat}_N\mathbb{C} \).
    Она субмультипликативна.
    При этом
    \( B \) состоит из \( M \) единиц, поэтому
    \[
        \beta = {\left\|B\right\|}_{F} =
        {\left\|B\right\|}_{F} = \sqrt{M}.
        \]
    Заметим также очевидное равенство
    \[
        \gamma = \frac1N
                \sup_{{\left\|X\right\|}_{F}=1}{\left\|\begin{pmatrix}0 & X_{12} \\ -X_{21} & 0\end{pmatrix}\right\|}_{F}
                = \frac1N. \]
    
    Если
     \( \sqrt{M} < \frac{N}{4} \),
     то выполняются условия леммы,
     прич\"ем \( r_0 < 4\sqrt{M} \).
    Это значит, что
     \( \sigma(A) = \sigma_1 \cup \sigma_2 \),
     где \( \sigma_1 = \{ \lambda_1 \}\subset\mathbb{R}, \lvert \lambda_1 - N \rvert < 4\sqrt{M} \),
     \( \sigma_2 \subset \{ \mu\in\mathbb{C}; \lvert\mu\rvert < 4\sqrt{M} \} \),
     \( \sigma_1 \cap \sigma_2 = \emptyset \).
    Теорема доказана.
    \end{proof}

\begin{thm}
    Suppose the number of absent edges is
    \[ M < \frac{1}{16} N^2. \]
    Then the spectrum of the adjacency matrix \( A = \mathcal{J}_N - B \)
        can be represented as disjoint union
    \[
        \sigma(A) = \{ N - x_{11}^o \} \cup \sigma_2.
    \]
    The dominating eigenvector of \( A \) is
    \[
        \hat{h}_N = U(E+\Gamma X^o) e_1 =
            h_N - \frac1N (X_{21,(1)}^o h_1 + \cdots + X_{21, (N{-}1)}^o h_{N{-}1}),
    \]
    where \( X_{21,(i)}^o,\ i=\overline{1,N{-}1} \) are the coordinates
    of the vector \( X_{21}^o \).
    Moreover \( \hat{h}_N\in\mathbb{R}^{N} \),
    \( x_{11}^o\in\mathbb{R} \) and \( \sigma_2\subset\mathbb{C} \)
    satisfy the following inequalities:
    \[
        \|\hat{h}_N - h_N\|_2 \leq 4\frac{\sqrt{M}}{N},
    \]
    \[
        \lvert x_{11}^o \rvert,
        \ \sup_{\lambda\in\sigma_2} \lvert\lambda\rvert \leq 4\sqrt{M}.
    \]
\end{thm}

% JPCS
\section{A-tiled matrix}

Now let \( A\in\mathtt{Mat}_{M{\times}M}(\mathbb{K}) \)
    and consider the following (``A-tiled'') block-matrix
    \[
        \mathbb{A} =
        \begin{pmatrix}
            A & \cdots & A \\
            \vdots & \ddots & \vdots \\
            A & \cdots & A
        \end{pmatrix}
        \in\mathbb{K}^{{MN}{\times}{MN}}
    \]
    and the perturbed matrix
    \[
        \mathbb{A} - \mathbb{B},\ \mathbb{B}\in\mathbb{K}^{{MN}{\times}{MN}}.
    \]

\begin{lem}
    Suppose \( A \) is an invertible self-adjoint matrix.
    Then it has \( M \) orthonormal eigenvectors \( h_1, \ldots, h_M \)
    (\(\left\|h_i\right\|_2 = 1,\ i{=}\overline{1,M}\))
    with the corresponding eigenvalues
    \( \lambda_1, \ldots, \lambda_M \neq 0\).
    The spectrum of \( \mathbb{A} \) is
    \[
        \sigma(\mathbb{A}) = \{0\}\cup N\sigma(A) = \{0\} \cup \{N\lambda;\ \lambda\in\sigma(A) \}.
    \]
    Non-zero eigenvalues of \( \mathbb{A} \)
        have corresponding block eigenvectors:
    \[
        f_j = \frac{1}{\sqrt{N}} (h_j, \ldots, h_j)\in \mathbb{K}^{MN},\ j=\overline{1,M}.
    \]
    The null-space of \( \mathbb{A} \)
        has an orthonormal basis:
    \[
        f_{j,k} = \frac{1}{\sqrt{k(k+1)}}
        (
        \underbrace{e_j, \ldots, e_j}_{k\ \text{copies}},
        -ke_j,
        0, \ldots, 0
        ) \in\mathbb{K}^{{MN}{\times}{MN}}
    \]

    The matrix \( \mathbb{A} \) is similar to a block-diagonal matrix:
    \[
        \mathcal{A} =
        \left(\begin{array}{c|c}
            \operatorname{diag}(N\lambda_1,\ldots,N\lambda_M) & \mathbf{0} \\ \hline
            \mathbf{0} & \mathbf{0}
        \end{array}\right)\in\mathbb{K}^{{MN}{\times}{MN}};
    \]
    the similarity transform matrix is
    \[
        U = \operatorname{columns}
        \left(f_1, \ldots, f_M, f_{1,1}, \ldots, f_{1,N{-1}}, \ldots, f_{M,N{-}1}\right).
    \]
\end{lem}

Throughout this subsection
    we will consider block matrices
    of the size \( {MN}{\times}{MN} \)
    in the form
    \[
    X =
        \left(\begin{array}{c|c}
            \begin{matrix}
                x_{11} & \cdots & x_{1M} \\
                \vdots & \ddots & \vdots \\
                x_{M1} & \cdots & x_{MM}
            \end{matrix} &
            \begin{matrix}
                x_{1,M+1} \\
                \vdots \\
                x_{M,M+1}
            \end{matrix} \\ \hline
            \begin{matrix}
                x_{M+1,1} &
                \cdots &
                x_{M+1,M}
            \end{matrix} &
            X_{M+1,M+1}
        \end{array}\right),
    \]
where
\( X_{ij}      {=} x_{ij},
 \ X_{M{+}1,j} {=} x_{M{+}1,j},
 \ X_{i,M{+}1} {=} x_{i,M{+}1} \in \mathbb{K} \)
for \( 1 \leq {i,j} \leq M \),
and
\( X_{M{+}1,M{+}1} \) is a block of the size \( {M(N{-}1){\times}M(N-1)} \).

Just like before we are going to investigate
    the spectral behaviour of \( \mathbb{A} \) under perturbations.
We begin with constructing an admissible triple.
A natural choice for \( J \) in this case is
\[
        J X =
        \left(\begin{array}{c|c}
            \begin{matrix}
                x_{11} &  & 0 \\
                 & \ddots &  \\
                0 &  & x_{MM}
            \end{matrix} &
            \begin{matrix}
                0 \\
                \vdots \\
                0
            \end{matrix} \\ \hline
            \begin{matrix}
                0 & \cdots & 0
            \end{matrix} &
            X_{M+1,M+1}
        \end{array}\right).
\]

\begin{lem}
Suppose \( A \) has a simple spectrum,
    i.e.\ its eigenvalues are pairwise distinct:
    \( \lambda_i\neq\lambda_j \) for all \( 1\leq i{\neq}j \leq M \).

    Then a tuple \( (\mathbb{K}^{{MN}{\times}{MN}}, J, \Gamma) \)
        forms an admissible triple if we define
    \[
        \Gamma X = 
        \frac1n \left(\begin{array}{c|c}
                        \Gamma_{11}(X) & \Gamma_{12}(X) \\ \hline
                        \Gamma_{21}(X) & \Gamma_{22}(X)
                      \end{array}\right),
                                      \]
    \[
        \Gamma_{11}(X) =
              \begin{pmatrix}
                0               & \gamma_{12}x_{12} & \cdots & \gamma_{1M}x_{1M} \\
                \gamma_{21}x_{21}  & 0              & \cdots & \gamma_{2M}x_{2M} \\
                \vdots          & \vdots         & \ddots & \vdots & \ \\
                \gamma_{M1}x_{M1}  & \gamma_{M2}x_{M2} & \cdots & 0
              \end{pmatrix}
    \]
    \[
        \Gamma_{12}(X) =
            \begin{pmatrix}
                \gamma_{1,M+1}x_{1,M+1} \\
                \gamma_{2,M+1}x_{2,M+1} \\
                \vdots \\
                \gamma_{M,M+1}x_{M,M+1}
            \end{pmatrix}
    \]
    \[
        \Gamma_{11}(X) =
            \begin{pmatrix}
                \gamma_{M{+}1,1}x_{M{+}1,1} &
                \gamma_{M{+}1,2}x_{M{+}1,2} &
                \cdots &
                \gamma_{M{+}1,M}x_{M{+}1,M}
            \end{pmatrix}
    \]
    \[
        \Gamma_{22}(X) = \mathbf{0},
    \]
    \[
        \gamma_{ij} = \left\{
            \begin{aligned}
                & \frac{1}{\lambda_i - \lambda_j},\ 1\leq i{\neq}j \leq M{+}1,\\
                & 0,\ i=j
            \end{aligned}
            \right.
    \]
    where we've used the convention:
    \[
        \lambda_{M{+}1} = 0.
    \]

    The operator norm of \( \Gamma \) is:
    \[
        \|\Gamma\|_{\mathrm{op}} =
        \frac1N
        \frac{1}{\min\limits_{1\leq i{\neq}j \leq M{+}1}|\lambda_i - \lambda_j|} =
        \]
    \[
        = \frac1N
         \max\left\{
         \frac{1}{
             \min\limits_{1\leq i{\neq}j \leq M }{|\lambda_i - \lambda_j|}},
         \frac{1}{
             \min\limits_{1\leq j \leq M}{|\lambda_j|}}
         \right\}
        \]
\end{lem}

\begin{thm}\label{nk:thm:tiled}
Suppose \( A \) has simple spectrum and the following inequality holds:
\[
    \left\| \mathbb{B} \right\|_{\mathrm{op}}
        \leq 
        \frac{N}{4}
         \min\left\{
             \min\limits_{1\leq i{\neq}j \leq M }{|\lambda_i - \lambda_j|},
             \min\limits_{1\leq j \leq M}{|\lambda_j|}
         \right\}.
 \]

Then the spectrum of disturbed matrix \( \mathbb{A} - \mathbb{B} \) is
\[
    \sigma\left(\mathbb{A}\right) =
        \left\{
            N\lambda_1 - x_{11}^o, \ldots, N\lambda_M - x_{MM}^o
        \right\}
    \cup \sigma_{M{+}1}.
\]

The eigenvectors
    \( \hat{f}_j,\ \hat{f}_{j,k},\ j{=}\overline{1,M},\ k{=}\overline{1,N{-1}} \)
    of the matrix \( \mathbb{A}{-}\mathbb{B} \),
    the values \( x_{jj}^o,\ j{=}\overline{1,M} \)
    and the set \( \sigma_{M{+}1} \) are in the following bounds:
\[
    \lvert x_{jj}^o\rvert,
    \ \max_{\lambda\in\sigma_{M{+}1}} \lvert\lambda\rvert
    \leq 4\|B\|,
\]
\[
    \left\| \hat{f}_j - f_j \right\|_2,
    \ \left\| \hat{f}_{j,k} - f_{j,k}\right\|_2
    \leq
    \frac4N \|B\|
         \max\left\{
         \frac{1}{
             \min\limits_{1\leq l{\neq}p \leq M }{|\lambda_l - \lambda_p|}},
         \frac{1}{
             \min\limits_{1\leq l \leq M}{|\lambda_l|}}
         \right\}
\]
for all \( j{=}\overline{1,M}, k{=}\overline{1,N-1} \).
\end{thm}

\section{Perturbed Kronecker products' spectra}

Now we consider Kronecker product
\[
    A\otimes B =
    \begin{pmatrix}
        a_{11} B & \cdots & a_{1N} B \\
        \vdots   & \ddots & \vdots \\
        a_{N1} B & \cdots & a_{NN} B
    \end{pmatrix}
    \in \mathbb{K}^{{MN}{\times}{MN}}
\]
of squared matrices
\( A={(a_{ij})}\in\mathtt{Mat}_{N{\times}N}(\mathbb{K}),
 \ B={(b_{ij})}\in\mathtt{Mat}_{M{\times}M}(\mathbb{K}). \)
We will analyze its spectral properties
    under small-norm perturbations:
\begin{equation}\label{-kronperturb}
    A\otimes B - F.
\end{equation}

% TODO:
% We will also address special
%     perturbations of the following form (cf.~\cite{XIANG2005210}):
% \[
%     (A-\Delta A)\otimes (B - \Delta B).
% \]

Kronecker product has several appealing properties~\cite{bellman-matrices-kron}.
\begin{itemize}
\item It is associative:
    \[ A\otimes (B\otimes C) = (A\otimes B)\otimes C. \]
\item It is distributive with respect to addition:
    \[ (A+B)\otimes(C+D) = A\otimes C + A\otimes D + B\otimes C + B\otimes D. \]
\item The Kronecker product of matrix products has the following property:
    \[ (AB)\otimes(CD) = (A\otimes C)(B\otimes D) \]
    whenever products \( AB \) and \( CD \) make sense.
\item The trace of \( A\otimes B \) is \[ \operatorname{tr}(A\otimes B) = \operatorname{tr}A\operatorname{tr}B. \]
\item If \( A \) and \( B \) are both symmetric matrices,
      then \( A\otimes B \) is symmetric as well.
\end{itemize}
Note also that the ``tiled'' matrix from the last example
    can be represented as a Kronecker product:
\[
    \mathbb{A} =
    \begin{pmatrix}
    A & \cdots & A\\
    \vdots & \ddots & \vdots \\
    A & \cdots & A\end{pmatrix} =
        \mathcal{J}_N\otimes A.
    \]

\begin{lem}
Suppose \( A \) and \( B \) have simple structure,
    i.e. \( A \) has \( N \) eigenvectors
    \( f_1, \ldots, f_N \)
    whose corresponding eigenvalues are \( \mu_1, \ldots, \mu_N \)
    and \( B \) has eigenvectors \( h_1, \ldots, h_M \)
    with the eigenvalues \( \lambda_1, \ldots, \lambda_M \).
Then \( A\otimes B \) also has simple structure;
    it has \( MN \) independent eigenvectors \( f_i\otimes h_j,\ i{=}\overline{1,N}, j{=}\overline{1,M} \)
    and the corresponding eigenvalues are \( \mu_i \lambda_j \).
\end{lem}


Now suppose that among these pairwise products \( \mu_i \lambda_j \)
    there are only \( s \) distinct values \( \nu_1, \ldots, \nu_s \).
To each eigenvalue \( \nu_k \) (\( k{=}\overline{1,s} \)) there corresponds
    an eigenspace \[ E_k = \operatorname{span}(f_i\otimes h_j;\ \mu_i\lambda_j = v_k,\ i{=}\overline{1,N},\ j{=}\overline{1,M}) \subset \mathbb{K}^{MN}. \]
These eigenspaces form a direct-sum decomposition of \( \mathbb{K}^{MN} \):
    \[ \mathbb{K}^{MN} = E_1 \oplus \cdots \oplus E_s. \]
Any vector \( x\in\mathbb{K}^{MN} \) can be uniquely represented
    in the form
    \begin{equation}\label{-decomposition-x}
        x = x_1 + \cdots + x_s,\ x_k\in E_k,\ k=\overline{1,s}.
    \end{equation}
This direct-sum decomposition of \( \mathbb{K}^{MN} \)
    corresponds to a decomposition of the identity matrix \( E\in \mathbb{K}^{MN{\times}MN} \)
    (which defines an identity operator)
    into a sum of matrices of the spectral projections:
    \[
        E = P_1 + \cdots + P_s.
    \]
The spectral projection \( \mathcal{P}_k \) (\(k{=}\overline{1,s}\)) is given by the formula
    \[
        \mathcal{P}_k x = x_k \in E_k\subset \mathbb{K}^{MN}
    \]
    with respect to the decomposition~\eqref{-decomposition-x} of \( x \).

For any matrix \( X\in \mathbb{K}^{MN{\times}MN} \)
    the following trivial equality holds:
    \[
        X = \sum_{i,j=1}^s P_i X P_j.
    \]

The matrix \( A\otimes B \) can be decomposed into
    \[
        \mathcal{A} = \sum v_j P_j.
    \]

Now we should be able to reproduce the same steps as before
    to retrieve the estimates.

% \begin{center}
% \textbf{Lemma.}
% {\it
The natural way to define \( J \) is as follows:
    \[
        JX = \sum_{j=1}^s P_j X P_j.
    \]
The system of equations
    \[\left\{\begin{aligned}
        & \mathcal{A}\Gamma X - (\Gamma X) \mathcal{A} = X - JX, \\
        & J\Gamma X = 0,\ X\in \mathbb{K}^{MN{\times}MN}
    \end{aligned}\right.\]
    has the unique solution:
    \[
        \Gamma X = \sum_{1\leq i{\neq}j \leq s} \frac{1}{\nu_i-\nu_j} P_i X P_j.
    \]
    The norm of \( \Gamma \) is:
    \[
        \|\Gamma\|_{\mathrm{op}} = \gamma = \frac{1}{\min_{1\leq i{\neq}j\leq s}\lvert\nu_i - \nu_j\rvert}
    \]
% \/}
% \end{center}


\begin{thm}\label{nk:thm:kron}
    Consider the perturbed matrix~\eqref{-kronperturb}
        \[
            A{\otimes}B - F.
        \]
    Let \( A\in\mathtt{Mat}_{N{\times}N}(\mathbb{K}) \) and \( B\in\mathbb{K}^{M{\times}M} \)
        be diagonalizable matrices.
    Let \( f_1, \ldots, f_N \) be the eigenvectors of \( A \)
        corresponding to the eigenvalues \( \mu_1, \ldots, \mu_N \)
        and let \( h_1, \ldots, h_M \) be the eigenvectors of \( B \)
        corresponding to the eigenvalues \( \lambda_1, \ldots, \lambda_M \).
    The spectrum of their Kronecker product \( A{\otimes}B \)
        is composed of all the possible pairwise products \( \mu_i \lambda_j \)
        and the corresponding eigenvectors are \( f_i\otimes h_j \).
    Suppose that out of these \( MN \) eigenvalues only \( s \) are distinct:
        \( \nu_1, \ldots \nu_s \).

    Suppose
    \[
        \|F\| \leq \frac14 \gamma^{-1} = \frac14 \min_{1\leq i{\neq}j\leq s}\lvert\nu_i - \nu_j\rvert.
    \]

    Then \( A{\otimes}B - F \) is similar to
    \[ \sum_{k=1}^s \nu_k P_k - JX^o = \sum_{k=1}^s (\nu_k P_k - P_k X^o P_k) \]
    for some \( X^o \in \mathbb{K}^{MN{\times}MN} \),
    \( \|X^o - F\|\leq 3\|F\| \).

    All the eigenvalues of \( A{\otimes}B - F \) are contained in the circles
    \[
        \Omega_k = \left\{
            \lambda\in\mathbb{C};
            \ \lvert\lambda - \nu_k\rvert \leq 4\|F\|
            \right\},
        \ k{=}\overline{1,s}.
    \]
    There is at least one eigenvalue in each of these circles.

    Suppose the eigenvalue \( \nu_k=\mu_{i_k}\lambda_{j_k} \) of \( A{\otimes}B \) has multiplicity \( 1 \),
        that is it has the only eigenvector \( v_k = f_{i_k}{\otimes}h_{j_k} \).
    It is equivalent to the statement that the eigenvalue \( \mu_{i_k} \)
        of \( A \) and the eigenvalue \( \lambda_{j_k} \) of \( B \)
        are both of multiplicity \( 1 \).
    Then \( A{\otimes}B - F \) has eigenvalue in the circle \( \Omega_k \)
        and the corresponding eigenvector \( \hat{v}_k \) is within bounds
    \[
        \|\hat{v}_k - v_k\| \leq 4\gamma \|F\|.
    \]
    If \( \nu_k \) is well separated from all the other eigenvalues of \( A{\otimes}B \):
    \[
        \min_{l\neq k}
        \lvert
        \nu_k - \nu_l
        \rvert
        \geq 4\|F\|,
    \]
    then \( \nu_k \) is the only eigenvalue of \( A{\otimes}B - F \)
    in that circle.
\end{thm}

For example, in the case of a ``tiled'' matrix
\[
    \mathcal{J}_N{\otimes}B =
    \begin{pmatrix}
        B & \cdots & B \\
        \vdots & \ddots & \vdots \\
        B & \cdots & B
    \end{pmatrix}
\]
    we would have
    \( \nu_1=N \),
    \( \nu_2=0 \).
Let \( \lambda_1,\ldots,\lambda_M \)
    be the eigenvalues of \( B \).
Spectrum of \( \mathcal{J}_N{\otimes}B \) is
    \[
        \sigma(\mathcal{J}_N{\otimes}B) = \left\{ \mu_i\lambda_j;\ i{=}\overline{1,2},\ j{=}\overline{1,M}\right\} = \{0\}\cup N\sigma(B).
    \]
All of these eigenvalues except for \( 0 \)
    are of multiplicity \( 1 \)
    and well-separated for sufficiently large \( N \).
Then \( \gamma=\frac1N \).
This directly implies the theorem of the previous section.

These results might be refined
    with the use of the theorem on splitting an operator~\cite{baskakov1987theorem}
    which allows to consider each eigenvalue individually
    and obtain more precise estimates for the corresponding
    eigenvalue of the perturbed matrix.
The higher the gap between a picked eigenvalue
    and the rest of the spectrum
    the higher the precision of the estimate
    for the corresponding eigenvalue of the perturbed matrix.

\section{Conclusion}

We have derived bounds
    for perturbations of Kronecker products
    of matrices using the method of similar operators
    which is a powerful tool in perturbation theory.
Developed in the context of Banach spaces
    this method turns out to be just as useful
    in finite-dimensional problems.

The problem of perturbation analysis for the Kronecker products
    despite being old still has a whole open field for future research.
An example of more general yet quite similar problem
    is that of an analysis of tensor products
    of finite-rank operators
    and abstract operators in Banach spaces.

\newpage
\appendix
\section{Definitions}

\textbf{Function} --- a triple \( (X, Y, f) \) of the domain set \( X \),
the target set \( Y \), and the rule \( f \) mapping each \( x\in X\)
into corresponding \( y\in Y \).

\textbf{Vector space over a field \(\mathbb{K}\) } --- a tuple \( (X, 0, +, \cdot) \)
of a set \( X \) which is an Abelian group with respect to addition \( +:X\to X\)
with identity \( 0\in X \) and a scaling (external multiplication by a scalar)
operation \(\cdot: \mathbb{K}\times X \to X \) with usual properties.
Sometimes we shall denote such space merely by \( X \).
Examples: coordinate spaces \( \mathbb{K}^n,\ n\in\mathbb{N} \) with elementwise
scaling and addition, polynomials of a given order \( k \) (i.e. of a degree
strictly less than \( k \)), etc.

\textbf{Linear operator} --- a mapping between vector spaces (over same field),
which ``respects'' and ``preserves'' their linear structure, i.e. such a mapping
\( A:X\to Y \) that \( A(\alpha x_1 + x_2) = \alpha Ax_1 + Ax_2,\
\alpha\in\mathbb{K}, x_i\in X \).

\textbf{Linear algebra} --- a vector space \( X \) with associative
multiplication operation \( (A, B)\mapsto AB: X^2\to X \).

\textbf{Inner product space}

(Directed multi-)\textbf{Graph} --- a triple \(G = (V, E, \phi)\)
of the set \( V \) of vertices, the set \( E \) of edges,
and the mapping \( \phi: E\to V^2\).

Graph is \textbf{undirected} if for each \( e\in E, \phi(e)=(u, v)\)
there is \( f\in E, \phi(f)=(v, u) \).

\textbf{Edge} --- an element \( e\in E \); if \( \phi(e) = (u, v)\in V^2 \)
we will say that ``\( e \) is an edge originating at \( u \) and terminating at \(
v \)'', or simply ``an edge from \( u \) to \( v \)''.

A vertex \( v \) is said to be \textbf{incident with} an edge \( e \)
if \( e \) originates (and) or terminates at \( v \).

Two vertices \( u, v\) are said to be \textbf{adjacent}
if they share same edge, i.e. if there is an edge \( e\in E \)
such that \( \phi(e)\in\{(u, v),\ (v, u) \}\).
\newpage
\section{References}
\nocite{*}
\printbibliography[heading=none]
\end{document}